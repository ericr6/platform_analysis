{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericr6/platform_analysis/blob/main/deal_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XX6HsFx4j1_6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRqpk_KzlI5W"
      },
      "source": [
        "# Task analysis of iExec MArketplace\n",
        "This notebook create dataframe using the graph,then from the dataframe you can easily create dashboard or explore data.\n",
        "\n",
        "Section 0 : import and define functions\n",
        "\n",
        "Section 1 : retrieve latest data, latest N days with parameter N\n",
        "\n",
        "Section 2 : Monitor hello world\n",
        "\n",
        "Section 3 : Platform Availability indicator\n",
        "\n",
        "Section 4 : Visualize recent activity\n",
        "\n",
        "Section 5 : Save historical data in google drive [DO NOT USE]\n",
        "\n",
        "Section 6 : Load full historical data from google drive\n",
        "\n",
        "Section 7 : Visualisation from historical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lpt0Ot5OSobr"
      },
      "source": [
        "## Section 0 : import and define functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcOCIr46QSfP"
      },
      "outputs": [],
      "source": [
        "# @title import stuffs\n",
        "import requests\n",
        "import json\n",
        "import csv\n",
        "from datetime import datetime, timedelta\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "from tabulate import tabulate\n",
        "\n",
        "try:\n",
        "    # Python 3.11+\n",
        "    from datetime import UTC\n",
        "except ImportError:\n",
        "    # fallback pour versions plus anciennes\n",
        "    from datetime import timezone\n",
        "    UTC = timezone.utc\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Configuration\n",
        "\n",
        "url_query_bellecour = 'https://thegraph.bellecour.iex.ec/subgraphs/name/bellecour/poco-v5'\n",
        "url_query_arbitrum = 'https://thegraph.arbitrum.iex.ec/api/subgraphs/id/B1comLe9SANBLrjdnoNTJSubbeC7cY7EoNu6zD82HeKy'\n",
        "\n",
        "workerpool_prod = '0x0e7bc972c99187c191a17f3cae4a2711a4188c3f'\n",
        "workerpool_debug = '0xdb214a4a444d176e22030be1ed89da1b029320f2'\n",
        "v8_learn_prod = '0x0975bfce90f4748dab6d6729c96b33a2cd5491f5'\n",
        "v8_learn_debug = '0xf900995aa41ab29bc16ba0785d7c67ad9d301296'\n",
        "wp_tdx = \"0x4568effcec8ba0787e52deef10ed03267e7c95b1\"\n",
        "\n",
        "workerpool_prod_arb=\"0x2c06263943180cc024daffeee15612db6e5fd248\"\n",
        "workerpool_debug_arb=\"0xaaa90d37034fd1ea27d5ef2879f217fb6fd7f7ca\"\n",
        "\n"
      ],
      "metadata": {
        "id": "13QN7W8cZK8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMaWDOIlj1_-"
      },
      "outputs": [],
      "source": [
        "# @title Load many Visualization functions\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "\n",
        "# Assuming your DataFrame is named 'df'\n",
        "# Convert DATE column to datetime if it's not already\n",
        "# Replace 'specific_workerpool_id' with the actual ID of the worker pool you're interested in\n",
        "\n",
        "def taskperday(df,workerpool_id, wpname):\n",
        "\n",
        "    # Filter the DataFrame further for the specific worker pool\n",
        "    df_taskday = df[df['WORKERPOOL ID'] == workerpool_id]\n",
        "    len(df_taskday)\n",
        "\n",
        "    df_taskday['DATE'] = pd.to_datetime(df_taskday['DATE'])\n",
        "    # Define the date interval\n",
        "    start_date = df_taskday['DATE'].min().strftime(\"%Y-%m-%d\")\n",
        "    end_date = df_taskday['DATE'].max().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    # Extract month from the DATE column\n",
        "    df_taskday['Day'] = df_taskday['DATE'].dt.to_period('d')\n",
        "\n",
        "    # Group by Month and STATUS, count occurrences\n",
        "    status_counts = df_taskday.groupby(['Day', 'STATUS']).size().unstack(fill_value=0)\n",
        "\n",
        "    # Plot\n",
        "    status_counts.plot(kind='bar', stacked=True, figsize=(20, 8))\n",
        "\n",
        "    plt.title('Tasks per day \\n (Workerpool: {}) \\n {} Start Date: {} | End Date: {}'.format(workerpool_id, wpname, start_date, end_date))\n",
        "    plt.xlabel('Month')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend(title='Status')\n",
        "\n",
        "    #Upscale resolution\n",
        "    plt.figure(dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "def taskplot(df,workerpool_filter, duration, wp_name):\n",
        "    end_date = datetime.now()\n",
        "    start_date = end_date - timedelta(days=duration)\n",
        "    date_filtered_df = df[(df['DATE'] >= start_date) & (df['DATE'] <= end_date)]\n",
        "    filtered_df = date_filtered_df[date_filtered_df['WORKERPOOL ID'] == workerpool_filter]\n",
        "\n",
        "    # Group by app name and date to count the number of uses of each dApp on each date\n",
        "    app_usage = filtered_df.groupby(['APP NAME', pd.Grouper(key='DATE', freq='D')]).size().unstack(fill_value=0)\n",
        "\n",
        "    # Get unique app names for y-axis ticks\n",
        "    dapp_names = app_usage.index\n",
        "\n",
        "    # Calculate figsize based on the number of dApps\n",
        "    fig_height = max(1, len(dapp_names) * 0.5)  # Minimum height of 6 inches\n",
        "    plt.figure(figsize=(25, fig_height))\n",
        "\n",
        "    # Plot usage events for each dApp\n",
        "    for i, app in enumerate(dapp_names):\n",
        "        usage_dates = filtered_df[filtered_df['APP NAME'] == app]['DATE']\n",
        "        plt.plot(usage_dates, [i] * len(usage_dates), marker='o', linestyle='', markersize=5, label=app)\n",
        "\n",
        "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
        "    end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
        "    plt.title('Usage of dApps Over Time \\n Workerpool: {} \\n {} \\n Last {} day(s)  [{} -> {}]'.format(wp_name,workerpool_filter, str(duration), start_date_str, end_date_str))\n",
        "\n",
        "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
        "    plt.ylabel('dApp')\n",
        "    plt.yticks(range(len(dapp_names)), dapp_names)  # Set y-axis ticks to be the unique app names\n",
        "    # Extend y-axis plot\n",
        "    plt.ylim(-0.5, len(dapp_names) - 0.5)\n",
        "\n",
        "  #plt.legend()  # Add legend to show dApp names\n",
        "\n",
        "    plt.grid(axis='y')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Success Rate per WorkerPool\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Function to calculate the percentage of successful tasks\n",
        "def calculate_success_percentage(total_tasks, successful_tasks):\n",
        "    if total_tasks == 0:\n",
        "        return 0\n",
        "    return (successful_tasks / total_tasks) * 100\n",
        "\n",
        "# Function to plot a pie chart with percentage and value\n",
        "def plot_pie_chart(ax, successful_tasks, total_tasks, title):\n",
        "    success_percentage = calculate_success_percentage(total_tasks, successful_tasks)\n",
        "    failed_tasks = total_tasks - successful_tasks\n",
        "\n",
        "    ax.pie([successful_tasks, failed_tasks], labels=[f'Successful Tasks ({successful_tasks})', f'Failed Tasks ({failed_tasks})'], autopct='%1.1f%%', startangle=140,colors=['lightgreen', 'lightcoral'])\n",
        "    ax.title(title + f' ({success_percentage:.2f}% success)', fontsize=12, fontweight='bold')\n",
        "    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
        "    ax.legend(loc='upper right', fontsize=10)\n",
        "\n",
        "def successrate(dftmp, workerpool_filter, duration, wp_name ):\n",
        "  # Calculate the date range for each time interval\n",
        "  # Replace 'specific_workerpool_id' with the actual ID of the worker pool you're interested in\n",
        "\n",
        "  end_date = datetime.now()\n",
        "  start_date = end_date - timedelta(days=duration)\n",
        "\n",
        "  # Filter the DataFrame for each time interval\n",
        "  df_datefiltered = dftmp[(dftmp['DATE'] >= start_date) & (dftmp['DATE'] <= end_date)]\n",
        "\n",
        "  # Filter the DataFrame further for the specific worker pool\n",
        "  df_tmp = df_datefiltered[df_datefiltered['WORKERPOOL ID'] == workerpool_filter]\n",
        "\n",
        "  # Calculate the total number of tasks and successful tasks for each time interval\n",
        "  total_tasks = len(df_tmp)\n",
        "  successful_tasks = len(df_tmp[df_tmp['STATUS'] == 'COMPLETED'])\n",
        "\n",
        "  # Create a figure with subplots\n",
        "#  fig, axs = plt.subplots(1, 4, figsize=(20, 8))\n",
        "  # Plot pie charts for each time interval\n",
        "\n",
        "  start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
        "  end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
        "  title=('Success Rate on {} \\n Workerpool: \\n {} \\n Last {} day(s) \\n [{} -> {}]'.format(wp_name,workerpool_prod, str(duration), start_date_str, end_date_str))\n",
        "\n",
        "  plot_pie_chart( plt, successful_tasks, total_tasks, title)\n",
        "\n",
        "  # Adjust layout and show plot\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "def success_repartition(df_tmp, workerpool, duration, wp_name):\n",
        "\n",
        "    # Filter the DataFrame for the last day\n",
        "    end_date = datetime.now()\n",
        "    start_date = end_date - timedelta(days=duration)\n",
        "    df_datefiltered = df_tmp[(df_tmp['DATE'] >= start_date) & (df_tmp['DATE'] <= end_date)]\n",
        "    # Filter the DataFrame further for the specific worker pool\n",
        "    df_wpfiltered = df_datefiltered[df_datefiltered['WORKERPOOL ID'] == workerpool]\n",
        "\n",
        "    # Create a dictionary to map DApp names to colors and marker types\n",
        "    dapp_info = {}\n",
        "    for i, dapp in enumerate(df_wpfiltered['APP NAME'].unique()):\n",
        "        dapp_info[dapp] = {\n",
        "            'color': plt.cm.tab20(i/len(df_wpfiltered['APP NAME'].unique())),\n",
        "            'marker': 'o'  # Use the same marker type for each DApp\n",
        "        }\n",
        "\n",
        "    # Plot successful and unsuccessful points using the same marker type per DApp\n",
        "    plt.figure(figsize=(20, 6))\n",
        "    for dapp, info in dapp_info.items():\n",
        "        dapp_df = df_wpfiltered[df_wpfiltered['APP NAME'] == dapp]\n",
        "        success_mask = dapp_df['STATUS'] == 'COMPLETED'\n",
        "\n",
        "        # Generate random y-coordinates for successful points in the range [0.1, 1]\n",
        "        random_success_y = np.random.uniform(low=0.1, high=1, size=np.sum(success_mask))\n",
        "        # Generate random y-coordinates for unsuccessful points in the range [-1, -0.1]\n",
        "        random_failure_y = np.random.uniform(low=-1, high=-0.1, size=np.sum(~success_mask))\n",
        "\n",
        "        # Plot successful points\n",
        "        plt.scatter(dapp_df['DATE'][success_mask], random_success_y, color=info['color'], label=f'{dapp}', marker=info['marker'])\n",
        "        # Plot unsuccessful points\n",
        "        plt.scatter(dapp_df['DATE'][~success_mask], random_failure_y, color=info['color'], marker=info['marker'])\n",
        "\n",
        "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
        "    end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
        "    title=('Tasks Success/fail distribution on \\n {}  \\n Workerpool: \\n {} \\n Last {} day(s) \\n [{} -> {}]'.format(wp_name, workerpool, str(duration), start_date_str, end_date_str))\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Task Status')\n",
        "    plt.yticks([-1, 0, 1], ['Failed', '', 'Success'])\n",
        "\n",
        "    # Create a single legend for both success and failure\n",
        "    plt.legend(title='DApp', loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def successrate_app(df_tmp, workerpool, wp_name):\n",
        "    # Filter the DataFrame to include only tasks associated with the specified workerpool\n",
        "    workerpool_df = df_tmp[df_tmp['WORKERPOOL ID'] == workerpool]\n",
        "\n",
        "    # Define the date interval\n",
        "    date_begin = df_tmp['DATE'].min().strftime(\"%Y-%m-%d\")\n",
        "    date_end = df_tmp['DATE'].max().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    # Calculate the success ratio for each DApp within the specified workerpool\n",
        "    dapp_data = {}\n",
        "    for dapp, group in workerpool_df.groupby('APP NAME'):\n",
        "        total_count = group.shape[0]\n",
        "        success_count = group[group['STATUS'] == 'COMPLETED'].shape[0]\n",
        "        ratio = success_count / total_count if total_count != 0 else 0\n",
        "        dapp_data[dapp] = {'ratio': ratio, 'success_count': success_count, 'total_count': total_count}\n",
        "\n",
        "    # Sort the dapp_data dictionary by the total_count of tasks\n",
        "    sorted_dapp_data = dict(sorted(dapp_data.items(), key=lambda item: item[1]['total_count'], reverse=False))\n",
        "\n",
        "    # Create a bar plot\n",
        "    fig_height = max(1, len(sorted_dapp_data) * 0.3)  # Calculate figsize based on the number of dApps\n",
        "    plt.figure(figsize=(25, fig_height))\n",
        "\n",
        "    plt.barh(list(sorted_dapp_data.keys()), [d['ratio'] for d in sorted_dapp_data.values()], color='lightgreen', label='Success Ratio')\n",
        "    plt.xlabel('Success Ratio')\n",
        "    plt.ylabel('DApp')\n",
        "    plt.title(f'Success Ratio of Tasks for Each DApp \\n ({date_begin} to {date_end}) \\n Workerpool {wp_name} : {workerpool}')\n",
        "    plt.legend()\n",
        "\n",
        "    # Add annotations for success ratio and total task count\n",
        "    for i, (dapp, data) in enumerate(sorted_dapp_data.items()):\n",
        "        plt.text(data['ratio'], i, f'{data[\"ratio\"]:.2f} {data[\"success_count\"]} out of {data[\"total_count\"]} tasks', verticalalignment='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Requester Activity Analysis for a defined app in last days\n",
        "\n",
        "def req_plot(df,workerpool_filter, duration, wp_name, app):\n",
        "    end_date = datetime.now()\n",
        "    start_date = end_date - timedelta(days=duration)\n",
        "    date_filtered_df = df[(df['DATE'] >= start_date) & (df['DATE'] <= end_date)]\n",
        "    filtered_df = date_filtered_df[date_filtered_df['WORKERPOOL ID'] == workerpool_filter]\n",
        "    filteredapp_df = filtered_df[filtered_df['APP NAME'] == app]\n",
        "\n",
        "\n",
        "    # Group by app name and date to count the number of uses of each dApp on each date\n",
        "    # req_usage = filteredapp_df.groupby(['REQUESTER ID', pd.Grouper(key='DATE', freq='D')]).size().unstack(fill_value=0)\n",
        "    # Define colors for different statuses\n",
        "    status_colors = {'COMPLETED': 'green', 'ACTIVE': 'red'}\n",
        "    address_counts = filteredapp_df['REQUESTER ID'].value_counts()\n",
        "\n",
        "\n",
        "    # Create a scatter plot with conditional coloring based on the status\n",
        "    plt.figure(figsize=(20, 14))\n",
        "    annotated_ids = set()  # Initialize a set to keep track of annotated IDs\n",
        "    for status, color in status_colors.items():\n",
        "        status_df = filteredapp_df[filteredapp_df['STATUS'] == status]\n",
        "        plt.scatter(status_df['DATE'], status_df['REQUESTER ID'], marker='o', color=color, label=status)\n",
        "\n",
        "        # Annotate points with the number of points for each address\n",
        "        for idx, row in status_df.iterrows():\n",
        "            if row['REQUESTER ID'] not in annotated_ids:  # Check if ID has not been annotated yet\n",
        "                plt.annotate(str(address_counts[row['REQUESTER ID']]), (row['DATE'], row['REQUESTER ID']), textcoords=\"offset points\", xytext=(-10,0), ha='right')\n",
        "                annotated_ids.add(row['REQUESTER ID'])  # Add ID to annotated set\n",
        "\n",
        "    # Set plot title and labels\n",
        "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
        "    end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
        "    plt.title('Scatter Plot of Requesters for \\n DApp {} \\n Workerpool: {} ; {}\\n Last {} day(s)    [{} -> {}]'.format(app, wp_name, workerpool_filter, duration, start_date_str, end_date_str))\n",
        "\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Requester')\n",
        "\n",
        "    # Rotate x-axis labels for better readability if needed\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Show plot\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Requester usage of specific app and workerpool in last days\n",
        "\n",
        "def req_count(df,workerpool_filter, duration, wp_name, app):\n",
        "    end_date = datetime.now()\n",
        "    start_date = end_date - timedelta(days=duration)\n",
        "    date_filtered_df = df[(df['DATE'] >= start_date) & (df['DATE'] <= end_date)]\n",
        "    filtered_df = date_filtered_df[date_filtered_df['WORKERPOOL ID'] == workerpool_filter]\n",
        "    filteredapp_df = filtered_df[filtered_df['APP NAME'] == app]\n",
        "\n",
        "\n",
        "    # Group by 'APP NAME' and 'STATUS' and count occurrences\n",
        "    app_counts = filteredapp_df.groupby(['REQUESTER ID', 'STATUS']).size()\n",
        "\n",
        "    # Reset index to make 'APP NAME' and 'STATUS' as columns\n",
        "    app_counts_df = app_counts.reset_index(name='Number of Occurrences')\n",
        "\n",
        "    # Pivot the DataFrame to have 'STATUS' as columns\n",
        "    app_counts_pivot = app_counts_df.pivot(index='REQUESTER ID', columns='STATUS', values='Number of Occurrences')\n",
        "\n",
        "    # Replace NaN values with zero\n",
        "    app_counts_pivot.fillna(0, inplace=True)\n",
        "\n",
        "    # Convert the DataFrame to use integer data type\n",
        "    app_counts_pivot = app_counts_pivot.astype(int)\n",
        "\n",
        "    # Add 'COMPLETED' column as sum of 'COMPLETED' and 'FAILED' columns\n",
        "    app_counts_pivot['COMPLETED'] = app_counts_pivot['COMPLETED']\n",
        "\n",
        "    # Sort the DataFrame based on 'COMPLETED' column\n",
        "    app_counts_pivot_sorted = app_counts_pivot.sort_values(by='COMPLETED', ascending=False)\n",
        "\n",
        "    # Print the DataFrame completely\n",
        "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
        "    end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
        "    print('Requester Activity for DApp {} on Workerpool: {} ; {}  Last {} day(s)    [{} -> {}]'.format(app, wp_name, workerpool_filter, duration, start_date_str, end_date_str))\n",
        "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # Display all rows and columns\n",
        "        print(app_counts_pivot_sorted)\n",
        "\n",
        "def req_count_with_detail(df, workerpool_filter, duration, wp_name, app):\n",
        "    end_date = datetime.now()\n",
        "    start_date = end_date - timedelta(days=duration)\n",
        "\n",
        "    # Filter data by date and workerpool\n",
        "    date_filtered_df = df[(df['DATE'] >= start_date) & (df['DATE'] <= end_date)]\n",
        "    filtered_df = date_filtered_df[date_filtered_df['WORKERPOOL ID'] == workerpool_filter]\n",
        "    filteredapp_df = filtered_df[filtered_df['APP NAME'] == app]\n",
        "\n",
        "    # First, get APP MULTIADDR analysis\n",
        "    app_multiaddr_analysis = filteredapp_df.groupby('APP MULTIADDR')['REQUESTER ID'].unique()\n",
        "\n",
        "    # Group by 'APP NAME' and 'STATUS' and count occurrences\n",
        "    app_counts = filteredapp_df.groupby(['REQUESTER ID', 'STATUS']).size()\n",
        "\n",
        "    # Reset index to make 'APP NAME' and 'STATUS' as columns\n",
        "    app_counts_df = app_counts.reset_index(name='Number of Occurrences')\n",
        "\n",
        "    # Pivot the DataFrame to have 'STATUS' as columns\n",
        "    app_counts_pivot = app_counts_df.pivot(index='REQUESTER ID', columns='STATUS', values='Number of Occurrences')\n",
        "\n",
        "    # Replace NaN values with zero\n",
        "    app_counts_pivot.fillna(0, inplace=True)\n",
        "\n",
        "    # Convert the DataFrame to use integer data type\n",
        "    app_counts_pivot = app_counts_pivot.astype(int)\n",
        "\n",
        "    # Add 'COMPLETED' column as sum of 'COMPLETED' and 'FAILED' columns\n",
        "    app_counts_pivot['COMPLETED'] = app_counts_pivot['COMPLETED']\n",
        "\n",
        "    # Add APP MULTIADDR column\n",
        "    requester_app_multiaddr = filteredapp_df.groupby('REQUESTER ID')['APP MULTIADDR'].first()\n",
        "    app_counts_pivot['APP MULTIADDR'] = app_counts_pivot.index.map(requester_app_multiaddr)\n",
        "\n",
        "    # Sort the DataFrame based on 'COMPLETED' column\n",
        "    app_counts_pivot_sorted = app_counts_pivot.sort_values(by='COMPLETED', ascending=False)\n",
        "\n",
        "    # Reorder columns to put APP MULTIADDR first\n",
        "    cols = app_counts_pivot_sorted.columns.tolist()\n",
        "    final_cols = ['APP MULTIADDR'] + [col for col in cols if col != 'APP MULTIADDR']\n",
        "    app_counts_pivot_sorted = app_counts_pivot_sorted[final_cols]\n",
        "\n",
        "    # Print the results\n",
        "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
        "    end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
        "    print('Requester Activity for DApp {} on Workerpool: {} ; {}  Last {} day(s)    [{} -> {}]'.format(\n",
        "        app, wp_name, workerpool_filter, duration, start_date_str, end_date_str))\n",
        "\n",
        "    # Print APP MULTIADDR analysis\n",
        "    print(\"\\nAPP MULTIADDR Analysis:\")\n",
        "    print(\"-\" * 50)\n",
        "    for multiaddr, requesters in app_multiaddr_analysis.items():\n",
        "        print(f\"\\nAPP MULTIADDR: {multiaddr}\")\n",
        "        print(\"Associated Requesters:\")\n",
        "        for requester in requesters:\n",
        "            print(f\"  - {requester}\")\n",
        "    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
        "\n",
        "    # Print the main activity table\n",
        "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "        print(\"Detailed Activity by Requester:\")\n",
        "        print(app_counts_pivot_sorted)\n",
        "\n",
        "    print('Requester Activity for DApp {} on Workerpool: {} ; {}  Last {} day(s)    [{} -> {}]'.format(app, wp_name, workerpool_filter, duration, start_date_str, end_date_str))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldOS_WJEjiTL"
      },
      "outputs": [],
      "source": [
        "# @title Load many Visualization functions\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.cm import get_cmap\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "from tabulate import tabulate\n",
        "from typing import Optional, List\n",
        "\n",
        "# Assuming your DataFrame is named 'df'\n",
        "# Convert DATE column to datetime if it's not already\n",
        "# Replace 'specific_workerpool_id' with the actual ID of the worker pool you're interested in\n",
        "\n",
        "def taskperday(df,workerpool_id, wpname):\n",
        "\n",
        "    # Filter the DataFrame further for the specific worker pool\n",
        "    df_taskday = df[df['WORKERPOOL ID'] == workerpool_id]\n",
        "    len(df_taskday)\n",
        "\n",
        "    df_taskday['DATE'] = pd.to_datetime(df_taskday['DATE'])\n",
        "    # Define the date interval\n",
        "    # Check if min/max dates are NaT before formatting\n",
        "    start_date_str = df_taskday['DATE'].min()\n",
        "    end_date_str = df_taskday['DATE'].max()\n",
        "\n",
        "    if pd.isna(start_date_str):\n",
        "        start_date_str = \"N/A\"\n",
        "    else:\n",
        "        start_date_str = start_date_str.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    if pd.isna(end_date_str):\n",
        "        end_date_str = \"N/A\"\n",
        "    else:\n",
        "        end_date_str = end_date_str.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "    # Extract month from the DATE column\n",
        "    df_taskday['Day'] = df_taskday['DATE'].dt.to_period('d')\n",
        "\n",
        "    # Group by Month and STATUS, count occurrences\n",
        "    status_counts = df_taskday.groupby(['Day', 'STATUS']).size().unstack(fill_value=0)\n",
        "\n",
        "    # Plot\n",
        "    status_counts.plot(kind='bar', stacked=True, figsize=(20, 8))\n",
        "\n",
        "    plt.title('Tasks per day \\n (Workerpool: {}) \\n {} Start Date: {} | End Date: {}'.format(workerpool_id, wpname, start_date_str, end_date_str))\n",
        "    plt.xlabel('Month')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend(title='Status')\n",
        "\n",
        "    #Upscale resolution\n",
        "    plt.figure(dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def taskplot(df,workerpool_filter, duration, wp_name):\n",
        "    end_date = datetime.now(UTC)\n",
        "    start_date = end_date - timedelta(days=duration)\n",
        "    date_filtered_df = df[(df['DATE'] >= start_date) & (df['DATE'] <= end_date)]\n",
        "    filtered_df = date_filtered_df[date_filtered_df['WORKERPOOL ID'] == workerpool_filter]\n",
        "\n",
        "    # Group by app name and date to count the number of uses of each dApp on each date\n",
        "    app_usage = filtered_df.groupby(['APP NAME', pd.Grouper(key='DATE', freq='D')]).size().unstack(fill_value=0)\n",
        "\n",
        "    # Get unique app names for y-axis ticks\n",
        "    dapp_names = app_usage.index\n",
        "\n",
        "    # Calculate figsize based on the number of dApps\n",
        "    fig_height = max(1, len(dapp_names) * 0.5)  # Minimum height of 6 inches\n",
        "    plt.figure(figsize=(25, fig_height))\n",
        "\n",
        "    # Plot usage events for each dApp\n",
        "    for i, app in enumerate(dapp_names):\n",
        "        usage_dates = filtered_df[filtered_df['APP NAME'] == app]['DATE']\n",
        "        plt.plot(usage_dates, [i] * len(usage_dates), marker='o', linestyle='', markersize=5, label=app)\n",
        "\n",
        "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
        "    end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
        "    plt.title('Usage of dApps Over Time \\n Workerpool: {} \\n {} \\n Last {} day(s)  [{} -> {}]'.format(wp_name,workerpool_filter, str(duration), start_date_str, end_date_str))\n",
        "\n",
        "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
        "    plt.ylabel('dApp')\n",
        "    plt.yticks(range(len(dapp_names)), dapp_names)  # Set y-axis ticks to be the unique app names\n",
        "    # Extend y-axis plot\n",
        "    plt.ylim(-0.5, len(dapp_names) - 0.5)\n",
        "\n",
        "  #plt.legend()  # Add legend to show dApp names\n",
        "\n",
        "    plt.grid(axis='y')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Success Rate per WorkerPool\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Function to calculate the percentage of successful tasks\n",
        "def calculate_success_percentage(total_tasks, successful_tasks):\n",
        "    if total_tasks == 0:\n",
        "        return 0\n",
        "    return (successful_tasks / total_tasks) * 100\n",
        "\n",
        "# Function to plot a pie chart with percentage and value\n",
        "def plot_pie_chart(ax, successful_tasks, total_tasks, title):\n",
        "    success_percentage = calculate_success_percentage(total_tasks, successful_tasks)\n",
        "    failed_tasks = total_tasks - successful_tasks\n",
        "\n",
        "    ax.pie([successful_tasks, failed_tasks], labels=[f'Successful Tasks ({successful_tasks})', f'Failed Tasks ({failed_tasks})'], autopct='%1.1f%%', startangle=140,colors=['lightgreen', 'lightcoral'])\n",
        "    ax.title(title + f' ({success_percentage:.2f}% success)', fontsize=12, fontweight='bold')\n",
        "    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
        "    ax.legend(loc='upper right', fontsize=10)\n",
        "\n",
        "\n",
        "def successrate(dftmp, workerpool_filter, duration, wp_name ):\n",
        "  # Calculate the date range for each time interval\n",
        "  # Replace 'specific_workerpool_id' with the actual ID of the worker pool you're interested in\n",
        "\n",
        "  end_date = datetime.now(UTC)\n",
        "  start_date = end_date - timedelta(days=duration)\n",
        "\n",
        "  # Filter the DataFrame for each time interval\n",
        "  df_datefiltered = dftmp[(dftmp['DATE'] >= start_date) & (dftmp['DATE'] <= end_date)]\n",
        "\n",
        "  # Filter the DataFrame further for the specific worker pool\n",
        "  df_tmp = df_datefiltered[df_datefiltered['WORKERPOOL ID'] == workerpool_filter]\n",
        "\n",
        "\n",
        "  # Calculate the total number of tasks and successful tasks for each time interval\n",
        "  total_tasks = len(df_tmp)\n",
        "  successful_tasks = len(df_tmp[df_tmp['STATUS'] == 'COMPLETED'])\n",
        "\n",
        "  # Create a figure with subplots\n",
        "#  fig, axs = plt.subplots(1, 4, figsize=(20, 8))\n",
        "  # Plot pie charts for each time interval\n",
        "\n",
        "  start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
        "  end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
        "  title=('Success Rate on {} \\n Workerpool: \\n {} \\n Last {} day(s) \\n [{} -> {}]'.format(wp_name,workerpool_prod, str(duration), start_date_str, end_date_str))\n",
        "\n",
        "  plot_pie_chart( plt, successful_tasks, total_tasks, title)\n",
        "\n",
        "  # Adjust layout and show plot\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "\n",
        "def _jitter(n, low, high, seed):\n",
        "    \"\"\"Retourne n valeurs aléatoires reproductibles entre low et high.\"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    return rng.uniform(low, high, size=n)\n",
        "\n",
        "def _jitter(n, low, high, seed):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    return rng.uniform(low, high, size=n)\n",
        "\n",
        "def success_repartition(\n",
        "        df: pd.DataFrame,\n",
        "        workerpool_id: str,\n",
        "        duration: int,\n",
        "        wp_name: str = \"\",\n",
        "        success_status: str = \"COMPLETED\",\n",
        "        fixed_alpha: bool = False      # << NOUVEL ARGUMENT\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Scatter succès / échec par dApp pour un workerpool.\n",
        "\n",
        "    fixed_alpha = True  ➜  y = ±α fixe par dApp\n",
        "    fixed_alpha = False ➜  y jitteré dans [0.1,1] / [-1,-0.1]\n",
        "    \"\"\"\n",
        "    # --- 1) filtre temporel + workerpool ----------------------------------\n",
        "    end_date, start_date = datetime.now(UTC), datetime.now(UTC) - timedelta(days=duration)\n",
        "    mask = (\n",
        "        (df['DATE'] >= start_date) &\n",
        "        (df['DATE'] <= end_date) &\n",
        "        (df['WORKERPOOL ID'] == workerpool_id)\n",
        "    )\n",
        "    data = df.loc[mask].copy()\n",
        "    if data.empty:\n",
        "        print(\"Aucune donnée pour les critères spécifiés.\")\n",
        "        return\n",
        "\n",
        "    # --- 2) palette & mapping dApp ----------------------------------------\n",
        "    dapps = data['APP NAME'].unique()\n",
        "    cmap  = get_cmap('tab20', len(dapps))\n",
        "    dapp_style = {app: dict(color=cmap(i), marker='o') for i, app in enumerate(dapps)}\n",
        "\n",
        "    # Si fixed_alpha : attribuer un α unique (équidistant) pour chaque dApp\n",
        "    if fixed_alpha:\n",
        "        alphas = np.linspace(0.2, 1.0, len(dapps), endpoint=True)\n",
        "        alpha_map = dict(zip(dapps, alphas))\n",
        "\n",
        "    # --- 3) figure ---------------------------------------------------------\n",
        "    height = max(4, len(dapps) * (0.4 if fixed_alpha else 0.25))\n",
        "    plt.figure(figsize=(20, height))\n",
        "    for app in dapps:\n",
        "        style = dapp_style[app]\n",
        "        sub   = data[data['APP NAME'] == app]\n",
        "        succ  = sub[sub['STATUS'] == success_status]\n",
        "        fail  = sub[sub['STATUS'] != success_status]\n",
        "\n",
        "        if fixed_alpha:\n",
        "            a = alpha_map[app]\n",
        "            y_succ = np.full(len(succ),  a)\n",
        "            y_fail = np.full(len(fail), -a)\n",
        "        else:\n",
        "            y_succ = _jitter(len(succ),  0.1, 1.0, seed=hash(app) & 0xFFFF)\n",
        "            y_fail = _jitter(len(fail), -1.0, -0.1, seed=~hash(app) & 0xFFFF)\n",
        "        plt.scatter(succ['DATE'], y_succ, color=style['color'],\n",
        "                    marker=style['marker'], label=app)\n",
        "        plt.scatter(fail['DATE'], y_fail, color=style['color'],\n",
        "                    marker=style['marker'])\n",
        "\n",
        "    # --- 4) titre, axes, légende ------------------------------------------\n",
        "    succ_pct = 100 * (data['STATUS'] == success_status).mean()\n",
        "    title = (\n",
        "        f\"Tasks success / fail – Workerpool: {wp_name or workerpool_id}\\n\"\n",
        "        f\"Fenêtre : {start_date:%Y-%m-%d} → {end_date:%Y-%m-%d} \"\n",
        "        f\"({duration} j) – Succès : {succ_pct:.1f}%\"\n",
        "    )\n",
        "    plt.title(title, fontsize=13)\n",
        "    plt.xlabel(\"Date / Heure\")\n",
        "    plt.ylabel(\"Status (α par dApp)\" if fixed_alpha else \"Status (jitter)\")\n",
        "    plt.yticks([-1, 0, 1], [\"Fail\", \"\", \"Success\"])\n",
        "    plt.grid(True, linestyle=\"--\", alpha=.3)\n",
        "\n",
        "    # légende unique par dApp\n",
        "    handles, labels = plt.gca().get_legend_handles_labels()\n",
        "    by_label = dict(zip(labels, handles))\n",
        "    plt.legend(by_label.values(), by_label.keys(),\n",
        "               title=\"DApps\", bbox_to_anchor=(1, 0.5), loc=\"center left\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def successrate_app(df_tmp, workerpool, wp_name):\n",
        "    # Filter the DataFrame to include only tasks associated with the specified workerpool\n",
        "    workerpool_df = df_tmp[df_tmp['WORKERPOOL ID'] == workerpool]\n",
        "\n",
        "    # Define the date interval\n",
        "    date_begin = df_tmp['DATE'].min()\n",
        "    date_end = df_tmp['DATE'].max()\n",
        "\n",
        "    if pd.isna(date_begin):\n",
        "        date_begin_str = \"N/A\"\n",
        "    else:\n",
        "        date_begin_str = date_begin.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    if pd.isna(date_end):\n",
        "        date_end_str = \"N/A\"\n",
        "    else:\n",
        "        date_end_str = date_end.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "    # Calculate the success ratio for each DApp within the specified workerpool\n",
        "    dapp_data = {}\n",
        "    for dapp, group in workerpool_df.groupby('APP NAME'):\n",
        "        total_count = group.shape[0]\n",
        "        success_count = group[group['STATUS'] == 'COMPLETED'].shape[0]\n",
        "        ratio = success_count / total_count if total_count != 0 else 0\n",
        "        dapp_data[dapp] = {'ratio': ratio, 'success_count': success_count, 'total_count': total_count}\n",
        "\n",
        "    # Sort the dapp_data dictionary by the total_count of tasks\n",
        "    sorted_dapp_data = dict(sorted(dapp_data.items(), key=lambda item: item[1]['total_count'], reverse=False))\n",
        "\n",
        "    # Create a bar plot\n",
        "    fig_height = max(10, len(sorted_dapp_data) * 0.15)  # Calculate figsize based on the number of dApps\n",
        "    plt.figure(figsize=(25, fig_height))\n",
        "\n",
        "    plt.barh(list(sorted_dapp_data.keys()), [d['ratio'] for d in sorted_dapp_data.values()], color='lightgreen', label='Success Ratio')\n",
        "    plt.xlabel('Success Ratio')\n",
        "    plt.ylabel('DApp')\n",
        "    plt.title(f'Success Ratio of Tasks for Each DApp \\n ({date_begin_str} to {date_end_str}) \\n Workerpool {wp_name} : {workerpool}')\n",
        "    plt.legend()\n",
        "\n",
        "    # Add annotations for success ratio and total task count\n",
        "    for i, (dapp, data) in enumerate(sorted_dapp_data.items()):\n",
        "        plt.text(data['ratio'], i, f'{data[\"ratio\"]:.2f} {data[\"success_count\"]} out of {data[\"total_count\"]} tasks', verticalalignment='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Requester Activity Analysis for a defined app in last days\n",
        "\n",
        "def req_plot(df,workerpool_filter, duration, wp_name, app):\n",
        "    end_date = datetime.now(UTC)\n",
        "    start_date = end_date - timedelta(days=duration)\n",
        "    date_filtered_df = df[(df['DATE'] >= start_date) & (df['DATE'] <= end_date)]\n",
        "    filtered_df = date_filtered_df[date_filtered_df['WORKERPOOL ID'] == workerpool_filter]\n",
        "    filteredapp_df = filtered_df[filtered_df['APP NAME'] == app]\n",
        "\n",
        "\n",
        "    # Group by app name and date to count the number of uses of each dApp on each date\n",
        "    # req_usage = filteredapp_df.groupby(['REQUESTER ID', pd.Grouper(key='DATE', freq='D')]).size().unstack(fill_value=0)\n",
        "    # Define colors for different statuses\n",
        "    status_colors = {'COMPLETED': 'green', 'ACTIVE': 'red'}\n",
        "    address_counts = filteredapp_df['REQUESTER ID'].value_counts()\n",
        "\n",
        "\n",
        "    # Create a scatter plot with conditional coloring based on the status\n",
        "    plt.figure(figsize=(20, 14))\n",
        "    annotated_ids = set()  # Initialize a set to keep track of annotated IDs\n",
        "    for status, color in status_colors.items():\n",
        "        status_df = filteredapp_df[filteredapp_df['STATUS'] == status]\n",
        "        plt.scatter(status_df['DATE'], status_df['REQUESTER ID'], marker='o', color=color, label=status)\n",
        "\n",
        "        # Annotate points with the number of points for each address\n",
        "        for idx, row in status_df.iterrows():\n",
        "            if row['REQUESTER ID'] not in annotated_ids:  # Check if ID has not been annotated yet\n",
        "                plt.annotate(str(address_counts[row['REQUESTER ID']]), (row['DATE'], row['REQUESTER ID']), textcoords=\"offset points\", xytext=(-10,0), ha='right')\n",
        "                annotated_ids.add(row['REQUESTER ID'])  # Add ID to annotated set\n",
        "\n",
        "    # Set plot title and labels\n",
        "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
        "    end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
        "    plt.title('Scatter Plot of Requesters for \\n DApp {} \\n Workerpool: {} ; {}\\n Last {} day(s)    [{} -> {}]'.format(app, wp_name, workerpool_filter, duration, start_date_str, end_date_str))\n",
        "\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Requester')\n",
        "\n",
        "    # Rotate x-axis labels for better readability if needed\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Show plot\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Requester usage of specific app and workerpool in last days\n",
        "\n",
        "def req_count(df,workerpool_filter, duration, wp_name, app):\n",
        "    end_date = datetime.now(UTC)\n",
        "    start_date = end_date - timedelta(days=duration)\n",
        "    date_filtered_df = df[(df['DATE'] >= start_date) & (df['DATE'] <= end_date)]\n",
        "    filtered_df = date_filtered_df[date_filtered_df['WORKERPOOL ID'] == workerpool_filter]\n",
        "    filteredapp_df = filtered_df[filtered_df['APP NAME'] == app]\n",
        "\n",
        "\n",
        "    # Group by 'APP NAME' and 'STATUS' and count occurrences\n",
        "    app_counts = filteredapp_df.groupby(['REQUESTER ID', 'STATUS']).size()\n",
        "\n",
        "    # Reset index to make 'APP NAME' and 'STATUS' as columns\n",
        "    app_counts_df = app_counts.reset_index(name='Number of Occurrences')\n",
        "\n",
        "    # Pivot the DataFrame to have 'STATUS' as columns\n",
        "    app_counts_pivot = app_counts_df.pivot(index='REQUESTER ID', columns='STATUS', values='Number of Occurrences')\n",
        "\n",
        "    # Replace NaN values with zero\n",
        "    app_counts_pivot.fillna(0, inplace=True)\n",
        "\n",
        "    # Convert the DataFrame to use integer data type\n",
        "    app_counts_pivot = app_counts_pivot.astype(int)\n",
        "\n",
        "    # Add 'COMPLETED' column as sum of 'COMPLETED' and 'FAILED' columns\n",
        "    app_counts_pivot['COMPLETED'] = app_counts_pivot['COMPLETED']\n",
        "\n",
        "    # Sort the DataFrame based on 'COMPLETED' column\n",
        "    app_counts_pivot_sorted = app_counts_pivot.sort_values(by='COMPLETED', ascending=False)\n",
        "\n",
        "    # Print the DataFrame completely\n",
        "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
        "    end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
        "    print('Requester Activity for DApp {} on Workerpool: {} ; {}  Last {} day(s)    [{} -> {}]'.format(app, wp_name, workerpool_filter, duration, start_date_str, end_date_str))\n",
        "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # Display all rows and columns\n",
        "        print(app_counts_pivot_sorted)\n",
        "\n",
        "\n",
        "def app_usage_per_day_tabulated(\n",
        "    df: pd.DataFrame,\n",
        "    duration: int = 30,\n",
        "    workerpool_filter: str | None = None,\n",
        "    wp_name: str | None = None,\n",
        "    top_k: int | None = 8,          # show only the top-K apps by total activity (None = all)\n",
        "    hide_zero_days: bool = True,    # drop days with no activity across shown apps\n",
        "    include_total: bool = True,     # add TOTAL column (ALL APPS, as C/O)\n",
        "    tablefmt: str = \"github\"        # 'psql', 'github', 'simple', etc.\n",
        "):\n",
        "    \"\"\"\n",
        "    Compact daily usage table:\n",
        "      - Rows = days (last `duration` days, inclusive of TODAY)\n",
        "      - Columns = one per app, each showing \"COMPLETED/OTHER\" (e.g., '12/3')\n",
        "      - Optional TOTAL column and top-K app selection\n",
        "\n",
        "    Expected df columns:\n",
        "      ['TASK_ID','APP NAME','APP MULTIADDR','TAG','STATUS','DATE','WORKERPOOL ID','REQUESTER ID']\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Time window (inclusive of TODAY) ---\n",
        "    # Use tomorrow 00:00 (exclusive) as the upper bound to include all of today.\n",
        "    tz = df['DATE'].dt.tz if pd.api.types.is_datetime64tz_dtype(df['DATE']) else None\n",
        "    now = pd.Timestamp.now(tz=tz)\n",
        "    end_day   = now.normalize()                           # today at 00:00\n",
        "    start_day = end_day - pd.Timedelta(days=duration - 1) # N days window, inclusive\n",
        "    next_day  = end_day + pd.Timedelta(days=1)            # tomorrow at 00:00 (exclusive upper bound)\n",
        "\n",
        "    # --- Base filtering ---\n",
        "    mask = (df['DATE'] >= start_day) & (df['DATE'] < next_day)\n",
        "    if workerpool_filter:\n",
        "        mask &= (df['WORKERPOOL ID'] == workerpool_filter)\n",
        "\n",
        "    sub = df.loc[mask, ['DATE', 'APP NAME', 'STATUS', 'TASK_ID']].copy()\n",
        "    if sub.empty:\n",
        "        print(f\"No data for the selected filters. Window [{start_day.date()} -> {end_day.date()}].\")\n",
        "        return\n",
        "\n",
        "    # Reduce STATUS to {COMPLETED, OTHER} and build the day key\n",
        "    sub['DAY'] = pd.to_datetime(sub['DATE']).dt.floor('D')\n",
        "    sub['STATUS2'] = np.where(sub['STATUS'] == 'COMPLETED', 'COMPLETED', 'OTHER')\n",
        "\n",
        "    # Group → MultiIndex columns: (APP NAME, STATUS2)\n",
        "    grouped = (\n",
        "        sub.groupby(['DAY', 'APP NAME', 'STATUS2'])['TASK_ID']\n",
        "           .count()\n",
        "           .unstack(['APP NAME', 'STATUS2'])\n",
        "           .fillna(0)\n",
        "           .astype(int)\n",
        "    )\n",
        "\n",
        "    # Ensure a full daily index from start_day .. end_day (inclusive)\n",
        "    all_days = pd.date_range(start=start_day, end=end_day, freq='D', tz=tz, name='DAY')\n",
        "    grouped = grouped.reindex(all_days, fill_value=0)\n",
        "\n",
        "    # Pick top-K apps by total (COMPLETED + OTHER) over the window\n",
        "    all_apps = sorted(set(grouped.columns.get_level_values(0)))\n",
        "    totals_per_app = {\n",
        "        app: grouped.loc[:, grouped.columns.get_level_values(0) == app].sum().sum()\n",
        "        for app in all_apps\n",
        "    }\n",
        "    if top_k is not None:\n",
        "        apps = [a for a, _ in sorted(totals_per_app.items(), key=lambda kv: kv[1], reverse=True)[:top_k]]\n",
        "    else:\n",
        "        apps = all_apps\n",
        "\n",
        "    # Helper to get a column safely (zeros if missing)\n",
        "    def get_status_series(app: str, status: str) -> pd.Series:\n",
        "        cols = [c for c in grouped.columns if c[0] == app and c[1] == status]\n",
        "        if cols:\n",
        "            return grouped[cols[0]]\n",
        "        return pd.Series(0, index=grouped.index, dtype=int)\n",
        "\n",
        "    # Build compressed display: each app = \"C/O\"\n",
        "    display = pd.DataFrame(index=grouped.index)\n",
        "    for app in apps:\n",
        "        c = get_status_series(app, 'COMPLETED')\n",
        "        o = get_status_series(app, 'OTHER')\n",
        "        display[app] = c.astype(str) + \"/\" + o.astype(str)\n",
        "\n",
        "    # TOTAL column (C/O) across all apps\n",
        "    if include_total:\n",
        "        comp_cols = grouped.loc[:, grouped.columns.get_level_values(1) == 'COMPLETED']\n",
        "        othr_cols = grouped.loc[:, grouped.columns.get_level_values(1) == 'OTHER']\n",
        "        comp_sum = (comp_cols.sum(axis=1) if not comp_cols.empty else pd.Series(0, index=grouped.index))\n",
        "        othr_sum = (othr_cols.sum(axis=1) if not othr_cols.empty else pd.Series(0, index=grouped.index))\n",
        "        display.insert(0, 'TOTAL', comp_sum.astype(int).astype(str) + \"/\" + othr_sum.astype(int).astype(str))\n",
        "\n",
        "    # Optionally drop days that are all zeros across shown apps\n",
        "    if hide_zero_days:\n",
        "        if include_total:\n",
        "            numeric_mask = (comp_sum + othr_sum) > 0\n",
        "        else:\n",
        "            # Sum only across shown apps/columns (both statuses)\n",
        "            shown_cols = [c for c in grouped.columns if c[0] in apps]\n",
        "            numeric_mask = (grouped[shown_cols].sum(axis=1) > 0) if shown_cols else pd.Series(False, index=grouped.index)\n",
        "        display = display.loc[numeric_mask]\n",
        "\n",
        "    # Pretty date column\n",
        "    display = display.copy()\n",
        "    # For tz-aware index, strftime uses the same tz; that’s fine for a day label\n",
        "    display.index = display.index.strftime(\"%Y-%m-%d\")\n",
        "    display.insert(0, \"DAY\", display.index)  # make DAY visible\n",
        "\n",
        "    # Header + print\n",
        "    wp_label = f\"{wp_name} ({workerpool_filter})\" if wp_name and workerpool_filter else (workerpool_filter or wp_name or \"ALL\")\n",
        "    print(f\"Daily app usage (COMPLETED/OTHER) — Workerpool: {wp_label} | Last {duration} day(s) \"\n",
        "          f\"[{start_day.date()} → {end_day.date()}]  (Top-{top_k if top_k else 'ALL'} apps)\")\n",
        "    print(tabulate(display.reset_index(drop=True), headers=\"keys\", tablefmt=tablefmt, showindex=False))\n",
        "\n",
        "\n",
        "def app_usage_per_day_by_requester_tabulated(\n",
        "    df: pd.DataFrame,\n",
        "    duration: int = 30,\n",
        "    workerpool_filter: Optional[str] = None,\n",
        "    wp_name: Optional[str] = None,\n",
        "    requesters: Optional[List[str]] = None,   # can be IDs or names\n",
        "    top_requesters: Optional[int] = None,\n",
        "    top_k_per_app: Optional[int] = 8,\n",
        "    hide_zero_days: bool = True,\n",
        "    include_total: bool = True,\n",
        "    tablefmt: str = \"github\",\n",
        "    show_name_column: bool = True,\n",
        "    show_address_in_label: bool = True\n",
        "):\n",
        "    \"\"\"\n",
        "    Prints daily usage tables per requester.\n",
        "    If a requester-name column exists (case-insensitive match to 'REQUESTER_ID_name'),\n",
        "    it will be used for headers and a REQUESTER column.\n",
        "    Required columns: DATE, APP NAME, STATUS, TASK_ID, REQUESTER ID\n",
        "    Optional: REQUESTER_ID_name (any casing/spaces)\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Hardcoded target name column (integrated here) ---\n",
        "    REQUESTER_NAME_COL_TARGET = \"REQUESTER ID_name\"\n",
        "\n",
        "    def _resolve_name_col_case_insensitive(df_: pd.DataFrame, target: str) -> Optional[str]:\n",
        "        tgt = target.strip().lower()\n",
        "        for c in df_.columns:\n",
        "            if c.strip().lower() == tgt:\n",
        "                return c\n",
        "        return None\n",
        "\n",
        "    # --- time window (inclusive of today) ---\n",
        "    tz = df['DATE'].dt.tz if pd.api.types.is_datetime64tz_dtype(df['DATE']) else None\n",
        "    now = pd.Timestamp.now(tz=tz)\n",
        "    end_day   = now.normalize()\n",
        "    start_day = end_day - pd.Timedelta(days=duration - 1)\n",
        "    next_day  = end_day + pd.Timedelta(days=1)\n",
        "\n",
        "    # --- base filter ---\n",
        "    mask = (df['DATE'] >= start_day) & (df['DATE'] < next_day)\n",
        "    if workerpool_filter:\n",
        "        mask &= (df['WORKERPOOL ID'] == workerpool_filter)\n",
        "\n",
        "    # detect requester-name column (case-insensitive)\n",
        "    detected_name_col = _resolve_name_col_case_insensitive(df, REQUESTER_NAME_COL_TARGET)\n",
        "    has_name = detected_name_col is not None\n",
        "\n",
        "    cols_needed = ['DATE', 'APP NAME', 'STATUS', 'TASK_ID', 'REQUESTER ID']\n",
        "    if has_name:\n",
        "        cols_needed.append(detected_name_col)\n",
        "\n",
        "    sub = df.loc[mask, cols_needed].copy()\n",
        "    if sub.empty:\n",
        "        print(f\"No data for the selected filters. Window [{start_day.date()} → {end_day.date()}].\")\n",
        "        return\n",
        "\n",
        "    # map {id -> name}\n",
        "    id2name = {}\n",
        "    if has_name:\n",
        "        tmpn = (sub[['REQUESTER ID', detected_name_col]].dropna(subset=['REQUESTER ID']))\n",
        "        tmpn = tmpn.loc[tmpn[detected_name_col].astype(str).str.strip() != \"\"]\n",
        "        tmpn = tmpn.drop_duplicates(subset=['REQUESTER ID'], keep='last')\n",
        "        id2name = dict(zip(tmpn['REQUESTER ID'], tmpn[detected_name_col]))\n",
        "\n",
        "    def requester_label(rid: str) -> str:\n",
        "        nm = id2name.get(rid)\n",
        "        if nm and str(nm).strip():\n",
        "            return f\"{nm} ({rid})\" if show_address_in_label else str(nm)\n",
        "        return rid\n",
        "\n",
        "    # normalize status/day\n",
        "    sub['DAY'] = pd.to_datetime(sub['DATE']).dt.floor('D')\n",
        "    sub['STATUS2'] = np.where(sub['STATUS'] == 'COMPLETED', 'COMPLETED', 'OTHER')\n",
        "\n",
        "    grouped = (\n",
        "        sub.groupby(['REQUESTER ID', 'DAY', 'APP NAME', 'STATUS2'])['TASK_ID']\n",
        "           .count()\n",
        "           .unstack(['APP NAME', 'STATUS2'])\n",
        "           .fillna(0)\n",
        "           .astype(int)\n",
        "    )\n",
        "    all_days = pd.date_range(start=start_day, end=end_day, freq='D', tz=tz, name='DAY')\n",
        "\n",
        "    # allow filtering by names or IDs\n",
        "    def name_or_id_to_id_list(req_list: List[str]) -> List[str]:\n",
        "        if not req_list:\n",
        "            return req_list\n",
        "        name2ids = {}\n",
        "        for rid, nm in id2name.items():\n",
        "            key = str(nm).strip().lower()\n",
        "            if key:\n",
        "                name2ids.setdefault(key, []).append(rid)\n",
        "\n",
        "        out, existing_ids = [], set(grouped.index.get_level_values(0))\n",
        "        for token in req_list:\n",
        "            tok = str(token).strip()\n",
        "            if tok in existing_ids:\n",
        "                out.append(tok); continue\n",
        "            key = tok.lower()\n",
        "            if key in name2ids:\n",
        "                out.extend(name2ids[key])\n",
        "\n",
        "        # unique preserving order\n",
        "        seen, res = set(), []\n",
        "        for x in out:\n",
        "            if x not in seen:\n",
        "                seen.add(x); res.append(x)\n",
        "        return res\n",
        "\n",
        "    if requesters is None:\n",
        "        totals_per_req = grouped.groupby(level='REQUESTER ID').sum().sum(axis=1)\n",
        "        req_order = [r for r, _ in sorted(totals_per_req.items(), key=lambda kv: kv[1], reverse=True)]\n",
        "        if top_requesters is not None:\n",
        "            req_order = req_order[:top_requesters]\n",
        "    else:\n",
        "        req_order = name_or_id_to_id_list(requesters)\n",
        "\n",
        "    wp_label = f\"{wp_name} ({workerpool_filter})\" if wp_name and workerpool_filter else (workerpool_filter or wp_name or \"ALL\")\n",
        "    header_common = f\"[{start_day.date()} → {end_day.date()}]  Workerpool: {wp_label}\"\n",
        "\n",
        "    for rid in req_order:\n",
        "        if rid not in grouped.index.get_level_values(0):\n",
        "            continue\n",
        "\n",
        "        req_df = grouped.loc[rid].reindex(all_days, fill_value=0)\n",
        "        req_df.index.name = 'DAY'\n",
        "\n",
        "        apps_all = sorted(set(req_df.columns.get_level_values(0)))\n",
        "        total_per_app = {app: req_df.loc[:, req_df.columns.get_level_values(0) == app].sum().sum()\n",
        "                         for app in apps_all}\n",
        "        apps = ( [a for a, _ in sorted(total_per_app.items(), key=lambda kv: kv[1], reverse=True)[:top_k_per_app]]\n",
        "                 if top_k_per_app is not None else apps_all )\n",
        "\n",
        "        def get_series(app: str, status: str) -> pd.Series:\n",
        "            cols = [c for c in req_df.columns if c[0] == app and c[1] == status]\n",
        "            return req_df[cols[0]] if cols else pd.Series(0, index=req_df.index, dtype=int)\n",
        "\n",
        "        display = pd.DataFrame(index=req_df.index)\n",
        "        for app in apps:\n",
        "            c = get_series(app, 'COMPLETED')\n",
        "            o = get_series(app, 'OTHER')\n",
        "            display[app] = c.astype(int).astype(str) + \"/\" + o.astype(int).astype(str)\n",
        "\n",
        "        comp_sum = pd.Series(0, index=req_df.index)\n",
        "        othr_sum = pd.Series(0, index=req_df.index)\n",
        "        if include_total:\n",
        "            comp_cols = req_df.loc[:, req_df.columns.get_level_values(1) == 'COMPLETED']\n",
        "            othr_cols = req_df.loc[:, req_df.columns.get_level_values(1) == 'OTHER']\n",
        "            if not comp_cols.empty: comp_sum = comp_cols.sum(axis=1)\n",
        "            if not othr_cols.empty: othr_sum = othr_cols.sum(axis=1)\n",
        "            display.insert(0, 'TOTAL', comp_sum.astype(int).astype(str) + \"/\" + othr_sum.astype(int).astype(str))\n",
        "\n",
        "        if hide_zero_days:\n",
        "            numeric_mask = (comp_sum + othr_sum) > 0 if include_total else (\n",
        "                req_df[[c for c in req_df.columns if c[0] in apps]].sum(axis=1) > 0 if apps else pd.Series(False, index=req_df.index)\n",
        "            )\n",
        "            display = display.loc[numeric_mask]\n",
        "\n",
        "        display = display.copy()\n",
        "        display.index = display.index.strftime(\"%Y-%m-%d\")\n",
        "        display.insert(0, \"DAY\", display.index)\n",
        "\n",
        "        if show_name_column:\n",
        "            display.insert(0, \"REQUESTER\", requester_label(rid))\n",
        "\n",
        "        if display.empty:\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n=== Daily app usage (COMPLETED/OTHER) — REQUESTER: {requester_label(rid)} — Last {duration} day(s) \"\n",
        "              f\"{header_common} (Top-{top_k_per_app if top_k_per_app else 'ALL'} apps) ===\")\n",
        "        print(tabulate(display.reset_index(drop=True), headers=\"keys\", tablefmt=tablefmt, showindex=False))\n",
        "\n",
        "\n",
        "def scatter_deals_by_dataset_v3(\n",
        "       df: pd.DataFrame,\n",
        "       dataset_name_filter: str | None = None,\n",
        "       duration: int = 30,\n",
        "       price_threshold: float = 0.0,\n",
        "       title: str | None = None,\n",
        "       id_len: int = 6,\n",
        "       show_no_deal: bool = True\n",
        "   ):\n",
        "   \"\"\"\n",
        "   Legend:\n",
        "– GREEN  : Successful task with paid dataset\n",
        "– YELLOW : Successful task with free dataset\n",
        "– ORANGE : Failed task with free dataset\n",
        "– RED    : Failed task with paid dataset\n",
        "   \"\"\"\n",
        "\n",
        "\n",
        "   # 1) fenêtre temporelle\n",
        "   end_date = datetime.utcnow()\n",
        "   start_date = end_date - timedelta(days=duration)\n",
        "   df_filt = df[(df['DATE'] >= start_date) & (df['DATE'] <= end_date)].copy()\n",
        "\n",
        "\n",
        "   # 2) filtre nom\n",
        "   if dataset_name_filter:\n",
        "       df_filt = df_filt[df_filt['DATASET_NAME'] == dataset_name_filter]\n",
        "\n",
        "\n",
        "   if df_filt.empty:\n",
        "       print(\"Aucune ligne après filtrage ; rien à tracer.\")\n",
        "       return\n",
        "\n",
        "\n",
        "   # 3) PLOT_DATE = DEAL_DATE si présent, sinon DATE\n",
        "   df_filt['PLOT_DATE'] = df_filt['DEAL_DATE'].where(df_filt['DEAL_DATE'].notna(), df_filt['DATE'])\n",
        "\n",
        "\n",
        "   # 4) Index Y = dataset ID → ligne\n",
        "   unique_ids = df_filt['DATASET_ID'].unique()\n",
        "   id_to_y = {ds: i for i, ds in enumerate(unique_ids)}\n",
        "   df_filt['Y_POS'] = df_filt['DATASET_ID'].map(id_to_y)\n",
        "\n",
        "\n",
        "   # 5) comptage\n",
        "   deal_counts = (\n",
        "       df_filt[df_filt['DEAL_ID'].notna()]\n",
        "       .groupby('DATASET_ID')['DEAL_ID']\n",
        "       .count()\n",
        "       .reindex(unique_ids, fill_value=0)\n",
        "   )\n",
        "\n",
        "\n",
        "   # 6) noms\n",
        "   names = df_filt.groupby('DATASET_ID')['DATASET_NAME'].first().reindex(unique_ids)\n",
        "\n",
        "\n",
        "   # 7) labels Y\n",
        "   y_labels = []\n",
        "   for ds in unique_ids:\n",
        "       name = names[ds]\n",
        "       name_part = f\" – {name}\" if pd.notnull(name) and name.strip() != \"\" else \"\"\n",
        "       count = deal_counts[ds]\n",
        "       label = f\"{ds}{name_part} ({count})\"\n",
        "       y_labels.append(label)\n",
        "\n",
        "\n",
        "   # 8) couleur combinée STATUS + PRICE\n",
        "   def determine_color(row):\n",
        "       price = row['DATASET_PRICE']\n",
        "       status = row.get('STATUS', 'UNKNOWN')\n",
        "       is_payant = pd.notnull(price) and float(price) > price_threshold\n",
        "       if status == 'COMPLETED':\n",
        "           return 'green' if is_payant else 'yellow'\n",
        "       else:\n",
        "           return 'red' if is_payant else 'orange'\n",
        "\n",
        "\n",
        "   df_filt['COLOR'] = df_filt.apply(determine_color, axis=1)\n",
        "\n",
        "\n",
        "   # 9) figure\n",
        "   plt.figure(figsize=(20, max(4, len(unique_ids) * 0.4)))\n",
        "\n",
        "\n",
        "   # 9-a) sans deal\n",
        "   if show_no_deal:\n",
        "       no_deal = df_filt[df_filt['DEAL_ID'].isna()]\n",
        "       if not no_deal.empty:\n",
        "           plt.scatter(\n",
        "               no_deal['PLOT_DATE'], no_deal['Y_POS'],\n",
        "               marker='D', color='grey', alpha=0.6, s=50,\n",
        "               label=f\"Sans deal (n={len(no_deal)})\"\n",
        "           )\n",
        "\n",
        "\n",
        "   # 9-b) avec deal (colorisé)\n",
        "   with_deal = df_filt[df_filt['DEAL_ID'].notna()]\n",
        "   if not with_deal.empty:\n",
        "       plt.scatter(\n",
        "           with_deal['PLOT_DATE'], with_deal['Y_POS'],\n",
        "           c=with_deal['COLOR'], alpha=0.8, marker='o',\n",
        "           label=\"Deals\"\n",
        "       )\n",
        "\n",
        "\n",
        "   # 10) mise en forme\n",
        "   if title is None:\n",
        "       ds_label = dataset_name_filter or 'Tous datasets'\n",
        "       title = f\"Deals par dataset – {ds_label}\\nPériode : {start_date.date()} → {end_date.date()}\"\n",
        "\n",
        "\n",
        "   plt.title(title, fontsize=14)\n",
        "   plt.xlabel('Date du deal (ou date dataset si aucun deal)')\n",
        "   plt.yticks(range(len(unique_ids)), y_labels)\n",
        "   plt.xticks(rotation=45)\n",
        "   plt.grid(True, linestyle='--', alpha=0.3)\n",
        "   plt.legend(title=\" GREEN  : Successful task with paid dataset \\n YELLOW : Successful task with free dataset \\n ORANGE : Failed task with free dataset \\n RED    : Failed task with paid dataset\")\n",
        "   plt.tight_layout()\n",
        "   plt.show()\n",
        "\n",
        "\n",
        "def get_tasks_by_deal(deal_id: str, endpoint: str = \"https://thegraph.bellecour.iex.ec/subgraphs/name/bellecour/poco-v5\"):\n",
        "   \"\"\"\n",
        "   Fetch task info by deal ID from iExec subgraph.\n",
        "   \"\"\"\n",
        "   query = f\"\"\"\n",
        "   {{\n",
        "     tasks(where: {{deal: \"{deal_id}\"}}) {{\n",
        "       id\n",
        "       deal {{ id }}\n",
        "       status\n",
        "     }}\n",
        "   }}\n",
        "   \"\"\"\n",
        "   response = requests.post(endpoint, json={\"query\": query})\n",
        "   response.raise_for_status()\n",
        "   data = response.json()\n",
        "   return data.get(\"data\", {}).get(\"tasks\", [])\n",
        "\n",
        "\n",
        "def enrich_df_with_status(df: pd.DataFrame, endpoint) -> pd.DataFrame:\n",
        "   \"\"\"\n",
        "   For each DEAL_ID in the df, fetch task(s) from thegraph and attach their status.\n",
        "   Returns a new dataframe with STATUS column.\n",
        "   \"\"\"\n",
        "   all_deal_ids = df['DEAL_ID'].dropna().unique()\n",
        "   status_map = {}\n",
        "\n",
        "\n",
        "   print(f\"Fetching status for {len(all_deal_ids)} unique deals...\")\n",
        "   for deal_id in all_deal_ids:\n",
        "       try:\n",
        "           tasks = get_tasks_by_deal(deal_id, endpoint)\n",
        "           if tasks:\n",
        "               # In case of multiple tasks, pick the latest or first\n",
        "               status_map[deal_id] = tasks[0]['status']\n",
        "           else:\n",
        "               status_map[deal_id] = None\n",
        "       except Exception as e:\n",
        "           print(f\"Error for deal {deal_id}: {e}\")\n",
        "           status_map[deal_id] = None\n",
        "\n",
        "\n",
        "   # Create new STATUS column\n",
        "   df['STATUS'] = df['DEAL_ID'].map(status_map)\n",
        "   return df\n",
        "\n",
        "\n",
        "def dataset_usage(dftmp8, title):\n",
        " tab=dftmp8.copy()\n",
        " tab.drop(\"DEAL_TIMESTAMP\", axis=1, inplace=True)\n",
        " tab.drop(\"DATASET_TIMESTAMP\", axis=1, inplace=True)\n",
        " tab.rename(columns={\"DATE\": \"DATASET_CREATE_DATE\"}, inplace=True)\n",
        " def move_column(df, col_name, new_index):\n",
        "     cols = list(df.columns)\n",
        "     cols.insert(new_index, cols.pop(cols.index(col_name)))\n",
        "     return df[cols]\n",
        "\n",
        "\n",
        " tab = move_column(tab, 'DATASET_CREATE_DATE', 0)\n",
        "\n",
        "\n",
        " # Ensure the DataFrame is sorted by the 'Group' column\n",
        " tab.sort_values(\"DATASET_CREATE_DATE\", inplace=True)\n",
        "\n",
        "\n",
        " # Create a list of rows with separators\n",
        " rows = []\n",
        " prev_group = None\n",
        "\n",
        "\n",
        " for _, row in tab.iterrows():\n",
        "     current_group = row[\"DATASET_ID\"]\n",
        "     if prev_group is not None and current_group != prev_group:\n",
        "         # Add separator only when group changes\n",
        "         rows.append([\"\"] * len(tab.columns))  # Or use: ['-'*10] * len(df.columns)\n",
        "     rows.append(row.tolist())\n",
        "     prev_group = current_group\n",
        "\n",
        "\n",
        " # Print with tabulate\n",
        " print(title)\n",
        " print(tabulate(rows, headers=tab.columns.tolist(), tablefmt=\"grid\"))\n",
        " return tab\n",
        "\n",
        "\n",
        "def summarize_owners(df: pd.DataFrame, top_n: int = 10) -> pd.DataFrame:\n",
        "   \"\"\"\n",
        "   Summarize dataset ownership and deal success.\n",
        "\n",
        "\n",
        "   Parameters\n",
        "   ----------\n",
        "   df      : DataFrame with at least\n",
        "             ['DATASET_OWNER', 'DATASET_ID', 'DEAL_ID']\n",
        "             If present, a 'STATUS' column (COMPLETED / FAIL / …)\n",
        "             will be used to compute real success-ratios.\n",
        "   top_n   : number of top owners to keep\n",
        "\n",
        "\n",
        "   Returns\n",
        "   -------\n",
        "   pd.DataFrame with columns\n",
        "       [Dataset Owner, Datasets Owned, Total Deals,\n",
        "        Successful Deals, Success Ratio (%)]\n",
        "   \"\"\"\n",
        "   # --- core metrics --------------------------------------------------------\n",
        "   owner_ds = (\n",
        "       df.groupby(\"DATASET_OWNER\")[\"DATASET_ID\"]\n",
        "         .nunique()\n",
        "         .reset_index(name=\"Datasets Owned\")\n",
        "   )\n",
        "\n",
        "\n",
        "   owner_deals = (\n",
        "       df.groupby(\"DATASET_OWNER\")[\"DEAL_ID\"]\n",
        "         .count()\n",
        "         .reset_index(name=\"Total Deals\")\n",
        "   )\n",
        "\n",
        "\n",
        "   # If STATUS exists → count real successes; else assume all succeed\n",
        "   if \"STATUS\" in df.columns:\n",
        "       successes = (\n",
        "           df.groupby(\"DATASET_OWNER\")[\"STATUS\"]\n",
        "             .apply(lambda s: (s.str.upper() == \"COMPLETED\").sum())\n",
        "             .reset_index(name=\"Successful Deals\")\n",
        "       )\n",
        "   else:\n",
        "       successes = owner_deals.rename(columns={\"Total Deals\": \"Successful Deals\"})\n",
        "\n",
        "\n",
        "   # ------------------------------------------------------------------------\n",
        "   summary = (\n",
        "       owner_ds.merge(owner_deals, on=\"DATASET_OWNER\")\n",
        "               .merge(successes, on=\"DATASET_OWNER\")\n",
        "   )\n",
        "\n",
        "\n",
        "   summary[\"Success Ratio (%)\"] = (\n",
        "       summary[\"Successful Deals\"] / summary[\"Total Deals\"] * 100\n",
        "   ).round(2)\n",
        "\n",
        "\n",
        "   # Friendly column names & ordering\n",
        "   summary = summary.rename(columns={\"DATASET_OWNER\": \"Dataset Owner\"})\n",
        "   summary = summary[\n",
        "       [\"Dataset Owner\", \"Datasets Owned\",\n",
        "        \"Total Deals\", \"Successful Deals\", \"Success Ratio (%)\"]\n",
        "   ]\n",
        "\n",
        "\n",
        "   return summary.sort_values(\n",
        "       [\"Datasets Owned\", \"Total Deals\"], ascending=False\n",
        "   ).head(top_n)\n",
        "\n",
        "def top_datasets_by_usage(df: pd.DataFrame, top_n: int = 5) -> pd.DataFrame:\n",
        "   \"\"\"\n",
        "   Returns the top N datasets by number of deals and success ratio.\n",
        "\n",
        "\n",
        "   Parameters:\n",
        "   - df: pandas DataFrame with columns:\n",
        "       ['DATASET_NAME', 'DATASET_ID', 'DATASET_OWNER', 'DEAL_ID']\n",
        "   - top_n: number of top datasets to return\n",
        "\n",
        "\n",
        "   Returns:\n",
        "   - DataFrame with renamed headers:\n",
        "       ['Dataset Name', 'Dataset Address', 'Dataset Owner',\n",
        "        'Total Deals', 'Successful Deals', 'Success Ratio (%)']\n",
        "   \"\"\"\n",
        "   grouped = (\n",
        "       df.groupby(['DATASET_NAME', 'DATASET_ID', 'DATASET_OWNER'])\n",
        "         .agg(total_deals=('DEAL_ID', 'count'))\n",
        "         .reset_index()\n",
        "   )\n",
        "\n",
        "\n",
        "   # Assume all deals are successful unless STATUS info is present\n",
        "   grouped['successful_deals'] = grouped['total_deals']\n",
        "   grouped['success_ratio (%)'] = (grouped['successful_deals'] / grouped['total_deals'] * 100).round(2)\n",
        "\n",
        "\n",
        "   # Rename columns for clarity\n",
        "   grouped = grouped.rename(columns={\n",
        "       'DATASET_NAME': 'Dataset Name',\n",
        "       'DATASET_ID': 'Dataset Address',\n",
        "       'DATASET_OWNER': 'Dataset Owner',\n",
        "       'total_deals': 'Total Deals',\n",
        "       'successful_deals': 'Successful Deals',\n",
        "       'success_ratio (%)': 'Success Ratio (%)'\n",
        "   })\n",
        "\n",
        "\n",
        "   return grouped.sort_values(by='Total Deals', ascending=False).head(top_n)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "a1tKgcvKg0fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzqFCDGYak1t"
      },
      "outputs": [],
      "source": [
        "# @title Common Functions\n",
        "\n",
        "import pandas as pd\n",
        "from typing import Dict, Union, List, Optional\n",
        "\n",
        "\n",
        "import time, json, random\n",
        "import requests\n",
        "from collections.abc import Sequence, Mapping\n",
        "from typing import Union, Optional\n",
        "\n",
        "import time, json, random\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# tqdm (optionnel) : pip install tqdm\n",
        "try:\n",
        "    from tqdm.auto import tqdm\n",
        "    _HAS_TQDM = True\n",
        "except Exception:\n",
        "    _HAS_TQDM = False\n",
        "\n",
        "GRAPHQL_HEADERS = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\"}\n",
        "\n",
        "def _post_graphql(session: requests.Session, endpoint: str, query: str, timeout: int):\n",
        "    try:\n",
        "        r = session.post(endpoint, headers=GRAPHQL_HEADERS, json={\"query\": query}, timeout=timeout)\n",
        "        if r.status_code != 200:\n",
        "            return None, f\"http {r.status_code}\"\n",
        "        try:\n",
        "            data = r.json()\n",
        "        except json.JSONDecodeError:\n",
        "            return None, \"json_decode_error\"\n",
        "        if isinstance(data, dict) and \"errors\" in data:\n",
        "            return None, f\"graphql_errors({len(data['errors'])})\"\n",
        "        return data, None\n",
        "    except Exception as e:\n",
        "        return None, f\"exc:{type(e).__name__}\"\n",
        "\n",
        "def _fetch_schema_one(session: requests.Session, endpoint: str, dataset_id: str,\n",
        "                      max_retries: int = 3, base_backoff: float = 0.6, timeout: int = 20):\n",
        "    q = f'query {{ protectedData(id:\"{dataset_id}\") {{ id jsonSchema }} }}'\n",
        "    for attempt in range(max_retries):\n",
        "        data, err = _post_graphql(session, endpoint, q, timeout)\n",
        "        if data and not err:\n",
        "            node = (data.get(\"data\") or {}).get(\"protectedData\")\n",
        "            if node and isinstance(node, dict):\n",
        "                return node.get(\"jsonSchema\")\n",
        "            return None\n",
        "        time.sleep(base_backoff * (attempt + 1) + random.uniform(0, 0.2))\n",
        "    return None\n",
        "\n",
        "def add_schema_from_chain(df: pd.DataFrame, endpoint: str, *,\n",
        "                          id_col: str = \"DATASET_ID\", out_col: str = \"schema\",\n",
        "                          max_retries: int = 3, timeout: int = 12, base_backoff: float = 0.3,\n",
        "                          sleep_between: float = 0.0, verbose: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"One-by-one, sans fallback, barre d’avancement, insertion juste après id_col.\"\"\"\n",
        "    if id_col not in df.columns:\n",
        "        raise KeyError(f\"Colonne '{id_col}' introuvable.\")\n",
        "\n",
        "    ids = df[id_col].tolist()\n",
        "    out = [None] * len(ids)\n",
        "\n",
        "    iterator = range(len(ids))\n",
        "    label = endpoint.split('/')[2] if '/' in endpoint else endpoint\n",
        "    if _HAS_TQDM:\n",
        "        iterator = tqdm(iterator, desc=f\"{out_col} @ {label}\", unit=\"id\")\n",
        "\n",
        "    misses = 0\n",
        "    with requests.Session() as session:\n",
        "        for i in iterator:\n",
        "            did = ids[i]\n",
        "            key = None if pd.isna(did) else str(did)\n",
        "            if key is None:\n",
        "                out[i] = None\n",
        "            else:\n",
        "                val = _fetch_schema_one(session, endpoint, key, max_retries, base_backoff, timeout)\n",
        "                out[i] = val\n",
        "                if val is None:\n",
        "                    misses += 1\n",
        "                if sleep_between > 0:\n",
        "                    time.sleep(sleep_between)\n",
        "\n",
        "    df2 = df.copy()\n",
        "    insert_at = list(df2.columns).index(id_col) + 1\n",
        "    df2.insert(insert_at, out_col, out)\n",
        "\n",
        "    if verbose:\n",
        "        found = pd.Series(out).notna().sum()\n",
        "        print(f\"{out_col}: {found} / {len(ids)} (manquants: {misses}) via {endpoint}\")\n",
        "    return df2\n",
        "\n",
        "\n",
        "\n",
        "def map_addresses(\n",
        "    df: pd.DataFrame,\n",
        "    columns: Union[str, Sequence[str]],\n",
        "    addr2name: Union[Mapping[str, str], pd.DataFrame],\n",
        "    *,\n",
        "    new_suffix: str = \"_name\",\n",
        "    inplace: bool = False,\n",
        "    case_insensitive: bool = True,\n",
        "    strip: bool = True,\n",
        "    key_col: str = None,    # only if addr2name is a DataFrame\n",
        "    value_col: str = None,  # only if addr2name is a DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    For each column in `columns`, creates a new '<col><new_suffix>' with the resolved name,\n",
        "    inserted immediately after <col> in the DataFrame.\n",
        "    - Address matching is case-insensitive by default (lowercase both sides).\n",
        "    - `addr2name` can be a dict {address -> name} or a DataFrame with key/value columns.\n",
        "    - Returns a *new* DataFrame unless `inplace=True`.\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(columns, str):\n",
        "        columns = [columns]\n",
        "\n",
        "    # Build a mapping dict from addr2name (supports dict OR DataFrame)\n",
        "    if isinstance(addr2name, pd.DataFrame):\n",
        "        if not key_col or not value_col:\n",
        "            raise ValueError(\"When addr2name is a DataFrame, you must provide key_col and value_col.\")\n",
        "        map_df = addr2name[[key_col, value_col]].dropna()\n",
        "        if case_insensitive:\n",
        "            keys = map_df[key_col].astype(str).str.lower()\n",
        "        else:\n",
        "            keys = map_df[key_col].astype(str)\n",
        "        values = map_df[value_col].astype(str)\n",
        "        mapping = dict(zip(keys, values))\n",
        "    else:\n",
        "        # assume Mapping\n",
        "        mapping = dict(addr2name)\n",
        "        if case_insensitive:\n",
        "            mapping = {str(k).lower(): v for k, v in mapping.items()}\n",
        "\n",
        "    out = df if inplace else df.copy()\n",
        "\n",
        "    for col in columns:\n",
        "        if col not in out.columns:\n",
        "            raise KeyError(f\"Column '{col}' not found in DataFrame.\")\n",
        "        # normalize source column\n",
        "        s = out[col].astype(str)\n",
        "        if strip:\n",
        "            s = s.str.strip()\n",
        "        s_norm = s.str.lower() if case_insensitive else s\n",
        "\n",
        "        # map to names\n",
        "        resolved = s_norm.map(mapping)\n",
        "\n",
        "        # insert immediately after the source column\n",
        "        new_col = f\"{col}{new_suffix}\"\n",
        "        insert_at = out.columns.get_loc(col) + 1\n",
        "        out.insert(insert_at, new_col, resolved)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# Function to convert Hexa String to Text\n",
        "def hex_to_string(value):\n",
        "    \"\"\"Decode 0x-hex UTF-8 strings; return None if not decodable.\"\"\"\n",
        "    if not value or not isinstance(value, str):\n",
        "        return None\n",
        "    v = value.strip()\n",
        "    if v.startswith(\"0x\"):\n",
        "        try:\n",
        "            return bytes.fromhex(v[2:]).decode(\"utf-8\", errors=\"ignore\")\n",
        "        except ValueError:\n",
        "            return None\n",
        "    return v  # already plain text\n",
        "\n",
        "\n",
        "# filtre temps (utilise UTC défini ci-dessus)\n",
        "def filter_by_time_interval(data_list, date_begin, date_end):\n",
        "    return [\n",
        "        row for row in data_list\n",
        "        if date_begin <= datetime.fromtimestamp(int(row[5]), UTC) <= date_end\n",
        "    ]\n",
        "\n",
        "def get_api_data_iexec_interval(url_query, query, global_skip, date_begin=None, date_end=None):\n",
        "    total_data, skip = [], 0\n",
        "    page_size = 500\n",
        "    max_skip  = 200000\n",
        "    detected  = False\n",
        "\n",
        "    # ⚑ precompute unix bounds once\n",
        "    begin_ts = int(date_begin.timestamp()) if date_begin else None\n",
        "    end_ts   = int(date_end.timestamp())   if date_end   else None\n",
        "\n",
        "    while skip < max_skip:\n",
        "        query_iter = query.replace(\"skip_param\", str(global_skip + skip))\n",
        "        options = {'headers': {'Content-Type': 'application/json'},\n",
        "                   'data': '{\"query\": \"' + query_iter + '\"}'}\n",
        "        r = requests.post(url_query, **options)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "\n",
        "        array_data = (data.get('data', {}) or {}).get('taskInitializes', []) or []\n",
        "        page_has_data = len(array_data) > 0\n",
        "\n",
        "        # Phase 1: keep hunting until we see at least one non-empty page\n",
        "        if not detected and not page_has_data:\n",
        "            skip += page_size\n",
        "            print(f\"i= {global_skip + skip} hunting... page empty\")\n",
        "            continue\n",
        "\n",
        "        # ⚑ If still no data ever found AND page is empty -> done\n",
        "        if detected and not page_has_data:\n",
        "            print(f\"i= {global_skip + skip} reached end (first empty after detection)\")\n",
        "            break\n",
        "\n",
        "        # ⚑ Inspect page timestamps to decide early stop\n",
        "        #    (API returns newest → oldest; if the NEWEST is already < begin_ts, we’re done)\n",
        "        if begin_ts is not None and page_has_data:\n",
        "            ts_list = [int(e.get('timestamp', 0)) for e in array_data]\n",
        "            page_newest = max(ts_list)  # since it's desc, this is typically first\n",
        "            page_oldest = min(ts_list)\n",
        "\n",
        "            # If even the newest < begin_ts => everything beyond is older -> stop\n",
        "            if page_newest < begin_ts:\n",
        "                print(f\"i= {global_skip + skip} stop (page_newest<{begin_ts})\")\n",
        "                break\n",
        "\n",
        "            # If entire page is > end_ts (too recent), just collect (filter will drop) and continue\n",
        "            # No special action needed here—this is just for your awareness:\n",
        "            # if end_ts is not None and page_oldest > end_ts:\n",
        "            #     pass\n",
        "\n",
        "        detected = True\n",
        "\n",
        "        # Build rows (dataset may be null)\n",
        "        grouped = [\n",
        "            [\n",
        "                e['task']['id'],\n",
        "                e['task']['deal']['app']['name'],\n",
        "                hex_to_string(e['task']['deal']['app']['multiaddr']),\n",
        "                e['task']['deal']['tag'],\n",
        "                e['task']['status'],\n",
        "                int(e.get('timestamp', 0)),\n",
        "                e['task']['deal']['workerpool']['id'],\n",
        "                e['task']['deal']['requester']['id'],\n",
        "                (e['task']['deal'].get('dataset') or {}).get('name'),\n",
        "                (e['task']['deal'].get('dataset') or {}).get('id'),\n",
        "            ]\n",
        "            for e in array_data\n",
        "        ]\n",
        "\n",
        "        # Date filter (does not affect pagination)\n",
        "        if date_begin is not None and date_end is not None:\n",
        "            grouped = filter_by_time_interval(grouped, date_begin, date_end)\n",
        "\n",
        "        total_data.extend(grouped)\n",
        "        skip += page_size\n",
        "\n",
        "        print(\"i= \" + str(global_skip + skip)\n",
        "              + \" detected: \" + str(detected)\n",
        "              + \" grouped size \" + str(len(grouped))\n",
        "              + \" array_data size: \" + str(len(array_data)))\n",
        "\n",
        "    return total_data\n",
        "\n",
        "\n",
        "# Actualize data\n",
        "\n",
        "query = '{\\\n",
        "                taskInitializes(first:500, skip: skip_param, orderBy: timestamp, orderDirection: desc){\\\n",
        "                  task{\\\n",
        "                    id,\\\n",
        "                    deal{\\\n",
        "                      requester {\\\n",
        "                        id\\\n",
        "                      }\\\n",
        "                      app{\\\n",
        "                        name\\\n",
        "                        multiaddr\\\n",
        "                      }\\\n",
        "                      dataset{\\\n",
        "                        name\\\n",
        "                        id\\\n",
        "                      }\\\n",
        "                      workerpool{\\\n",
        "                        id\\\n",
        "                         }\\\n",
        "                      tag\\\n",
        "                    }\\\n",
        "                    status,\\\n",
        "                  }\\\n",
        "                  timestamp\\\n",
        "                }\\\n",
        "              }'\n",
        "\n",
        "def get_api_datasets_interval(\n",
        "        url_query: str,\n",
        "        global_skip: int = 0,\n",
        "        date_begin: int | None = None,\n",
        "        date_end: int | None = None,\n",
        "        first: int = 1000\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Récupère les datasets avec leurs usages (s'il y en a), paginés.\n",
        "\n",
        "    Chaque ligne représente un couple (dataset, usage) ou un dataset seul.\n",
        "\n",
        "    Retourne :\n",
        "        [\n",
        "            dataset_id,\n",
        "            dataset_name,\n",
        "            owner_id,\n",
        "            dataset_timestamp,\n",
        "            usage_id (ou None),\n",
        "            dataset_price (ou None),\n",
        "            usage_timestamp (ou None)\n",
        "        ]\n",
        "    \"\"\"\n",
        "\n",
        "    def filter_by_time_interval(rows):\n",
        "        if date_begin is None and date_end is None:\n",
        "            return rows\n",
        "        return [\n",
        "            r for r in rows\n",
        "            if (date_begin is None or r[3] >= date_begin) and\n",
        "               (date_end   is None or r[3] <= date_end)\n",
        "        ]\n",
        "\n",
        "    total_data = []\n",
        "    detected = False\n",
        "    are_data = True\n",
        "    skip = 0\n",
        "\n",
        "    while are_data and (skip < 20000 or not detected):\n",
        "        query = f\"\"\"\n",
        "        {{\n",
        "          datasets(first:{int(first)}, skip:{int(global_skip) + int(skip)},\n",
        "                   orderBy:timestamp, orderDirection:desc) {{\n",
        "            id\n",
        "            owner {{ id }}\n",
        "            timestamp\n",
        "            name\n",
        "            usages(first:1000, orderBy:timestamp, orderDirection:desc) {{\n",
        "              id\n",
        "              datasetPrice\n",
        "              timestamp\n",
        "            }}\n",
        "          }}\n",
        "        }}\n",
        "        \"\"\"\n",
        "\n",
        "        opts = {\n",
        "            \"headers\": {\"Content-Type\": \"application/json\"},\n",
        "            \"json\": {\"query\": query}\n",
        "        }\n",
        "        resp = requests.post(url_query, **opts)\n",
        "        resp.raise_for_status()\n",
        "        payload = resp.json()\n",
        "\n",
        "        page = payload.get(\"data\", {}).get(\"datasets\", [])\n",
        "\n",
        "        flat_rows = []\n",
        "        for d in page:\n",
        "            dataset_id = d[\"id\"]\n",
        "            dataset_name = d.get(\"name\", \"\")\n",
        "            owner_id = d[\"owner\"][\"id\"]\n",
        "            dataset_ts = int(d[\"timestamp\"])\n",
        "\n",
        "            usages = d.get(\"usages\", [])\n",
        "\n",
        "            if not usages:\n",
        "                # Ajouter quand aucun usage n'existe\n",
        "                flat_rows.append([\n",
        "                    dataset_id,\n",
        "                    dataset_name,\n",
        "                    owner_id,\n",
        "                    dataset_ts,\n",
        "                    None,  # usage id\n",
        "                    None,  # datasetPrice\n",
        "                    None   # usage timestamp\n",
        "                ])\n",
        "            else:\n",
        "                for u in usages:\n",
        "                    flat_rows.append([\n",
        "                        dataset_id,\n",
        "                        dataset_name,\n",
        "                        owner_id,\n",
        "                        dataset_ts,\n",
        "                        u[\"id\"],\n",
        "                        float(u[\"datasetPrice\"]),\n",
        "                        int(u[\"timestamp\"])\n",
        "                    ])\n",
        "\n",
        "        flat_rows = filter_by_time_interval(flat_rows)\n",
        "\n",
        "        total_data.extend(flat_rows)\n",
        "        detected |= bool(flat_rows)\n",
        "        are_data = bool(page)\n",
        "        skip += first\n",
        "\n",
        "        print(\n",
        "            f\"i={global_skip + skip:6d}  \"\n",
        "            f\"page_raw={len(page):4d}  \"\n",
        "            f\"rows_kept={len(flat_rows):4d}  \"\n",
        "            f\"total={len(total_data):6d}\"\n",
        "        )\n",
        "\n",
        "    return total_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5HB6pdHSwh8"
      },
      "source": [
        "## Section 1 : retrieve latest data Bellecour\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gq5Bq8KYUPXS",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title Get latest deal ; Select N Days\n",
        "\n",
        "\n",
        "#Method 2: with Date\n",
        "date_end = datetime.now(UTC)\n",
        "date_begin = date_end - timedelta(days=37)\n",
        "print(\"Period to retrieve:\", date_begin, date_end)\n",
        "\n",
        "api_data = get_api_data_iexec_interval(url_query_bellecour, query, 0, date_begin, date_end)\n",
        "df_tasks_30d = pd.DataFrame(api_data, columns=[\"TASK_ID\",\"APP NAME\", \"APP MULTIADDR\", \"TAG\", \"STATUS\", \"DATE\", \"WORKERPOOL ID\", \"REQUESTER ID\", \"DATASET_NAME\", \"DATASET_ID\"])\n",
        "df_tasks_30d[\"DATE\"] = (\n",
        "    df_tasks_30d[\"DATE\"]\n",
        "    .astype(int)\n",
        "    .apply(lambda x: datetime.fromtimestamp(x, UTC))\n",
        ")\n",
        "\n",
        "print(\"Period observed on bellecour\", df_tasks_30d[\"DATE\"].min(), df_tasks_30d[\"DATE\"].max())\n",
        "print(\"dataset length: \" + str(df_tasks_30d.shape[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_tasks_30d.head()\n"
      ],
      "metadata": {
        "id": "ubKaIncUseKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR-wa-SSlr9T"
      },
      "source": [
        "Check dataset structure and data header, you can share to chatgpt this data structure, and ask him to plot anything you want, he will propose the code, then create a computing box and insert the code directly here to obtain what you ask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L-sDc_T-S3B"
      },
      "outputs": [],
      "source": [
        "# @title check data set structure\n",
        "\n",
        "df_tasks_30d.dtypes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Get latest dataset ; Select N Days\n",
        "\n",
        "#Method 2: with Date\n",
        "date_end = datetime.now()\n",
        "date_begin = date_end - timedelta(days=30)\n",
        "date_begin = int(date_begin.timestamp())\n",
        "date_end = int(date_end.timestamp())\n",
        "\n",
        "# Convert to Unix timestamps (seconds)\n",
        "print (\"Period to retrieve:\", date_begin, date_end)\n",
        "api_dataset = get_api_datasets_interval(url_query_bellecour, 0, date_begin, date_end)\n",
        "df_dataset_30d = pd.DataFrame(api_dataset, columns=[\"DATASET_ID\",\"DATASET_NAME\",\"DATASET_OWNER\",\"DATASET_TIMESTAMP\", \"DEAL_ID\", \"DATASET_PRICE\",\"DEAL_TIMESTAMP\"])\n",
        "df_dataset_30d[\"DATE\"] = [datetime.utcfromtimestamp(int(x)) for x in df_dataset_30d[\"DATASET_TIMESTAMP\"]]\n",
        "df_dataset_30d[\"DEAL_DATE\"] = pd.to_datetime(df_dataset_30d[\"DEAL_TIMESTAMP\"], unit='s', errors='coerce')\n",
        "\n",
        "print(\"Period observed\", df_dataset_30d[\"DATE\"].min(), df_dataset_30d[\"DATE\"].max())\n",
        "print(\"dataset length: \" + str(df_dataset_30d.shape[0]))\n"
      ],
      "metadata": {
        "id": "gH19ip7F_YXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # @title check dataset structure\n",
        "\n",
        "df_dataset_30d.dtypes"
      ],
      "metadata": {
        "id": "1V66lGYIqXv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Add schema to datasets\n",
        "df_dataset_30d = add_schema_from_chain(\n",
        "    df_dataset_30d,\n",
        "    endpoint= \"https://thegraph.iex.ec/subgraphs/name/bellecour/dataProtector-v2\",\n",
        "    id_col=\"DATASET_ID\",\n",
        "    out_col=\"schema\",\n",
        "    max_retries=2,\n",
        "    timeout=40,\n",
        "    base_backoff=0.8,\n",
        "    sleep_between=0.0,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "f1Utd-53ry5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wun9jqSh1NDW"
      },
      "source": [
        "## Section 1 bis: retrieve latest data Arbitrum\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCXYr2Mt1NDX"
      },
      "outputs": [],
      "source": [
        "# @title Get latest deal ; Select N Days\n",
        "\n",
        "\n",
        "#Method 2: with Date\n",
        "date_end = datetime.now(UTC)\n",
        "date_begin = date_end - timedelta(days=36)\n",
        "print(\"Period to retrieve:\", date_begin, date_end)\n",
        "\n",
        "api_data = get_api_data_iexec_interval(url_query_arbitrum, query, 0, date_begin, date_end)\n",
        "df_tasks_30d_arb = pd.DataFrame(api_data, columns=[\"TASK_ID\",\"APP NAME\", \"APP MULTIADDR\", \"TAG\", \"STATUS\", \"DATE\", \"WORKERPOOL ID\", \"REQUESTER ID\", \"DATASET_NAME\", \"DATASET_ID\"])\n",
        "df_tasks_30d_arb[\"DATE\"] = (\n",
        "    df_tasks_30d_arb[\"DATE\"]\n",
        "    .astype(int)\n",
        "    .apply(lambda x: datetime.fromtimestamp(x, UTC))\n",
        ")\n",
        "print(\"Period observed on arb\", df_tasks_30d_arb[\"DATE\"].min(), df_tasks_30d_arb[\"DATE\"].max())\n",
        "print(\"dataset length: \" + str(df_tasks_30d_arb.shape[0]))\n",
        "\n",
        "#print(filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_tasks_30d_arb.head()\n"
      ],
      "metadata": {
        "id": "mMAsxNKN1NDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-wMKNRl1NDY"
      },
      "source": [
        "Check dataset structure and data header, you can share to chatgpt this data structure, and ask him to plot anything you want, he will propose the code, then create a computing box and insert the code directly here to obtain what you ask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80Hkf4-l1NDY"
      },
      "outputs": [],
      "source": [
        "# @title check data set structure\n",
        "\n",
        "df_tasks_30d_arb.dtypes\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Get latest dataset ; Select N Days\n",
        "\n",
        "\n",
        "#Method 2: with Date\n",
        "date_end = datetime.now()\n",
        "date_begin = date_end - timedelta(days=30)\n",
        "date_begin = int(date_begin.timestamp())\n",
        "date_end = int(date_end.timestamp())\n",
        "\n",
        "# Convert to Unix timestamps (seconds)\n",
        "print (\"Period to retrieve:\", date_begin, date_end)\n",
        "\n",
        "api_dataset = get_api_datasets_interval(url_query_arbitrum, 0, date_begin, date_end)\n",
        "df_dataset_30d_arb = pd.DataFrame(api_dataset, columns=[\"DATASET_ID\",\"DATASET_NAME\",\"DATASET_OWNER\",\"DATASET_TIMESTAMP\", \"DEAL_ID\", \"DATASET_PRICE\",\"DEAL_TIMESTAMP\"])\n",
        "df_dataset_30d_arb[\"DATE\"] = [datetime.utcfromtimestamp(int(x)) for x in df_dataset_30d_arb[\"DATASET_TIMESTAMP\"]]\n",
        "df_dataset_30d_arb[\"DEAL_DATE\"] = pd.to_datetime(df_dataset_30d_arb[\"DEAL_TIMESTAMP\"], unit='s', errors='coerce')\n",
        "\n",
        "print(\"Period observed\", df_dataset_30d_arb[\"DATE\"].min(), df_dataset_30d_arb[\"DATE\"].max())\n",
        "print(\"dataset length: \" + str(df_dataset_30d_arb.shape[0]))\n",
        "\n"
      ],
      "metadata": {
        "id": "mNdwOQro1NDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Add schema to datasets\n",
        "\n",
        "df_dataset_30d_arb = add_schema_from_chain(\n",
        "    df_dataset_30d_arb,\n",
        "    endpoint=\"https://thegraph.arbitrum.iex.ec/api/subgraphs/id/Ep5zs5zVr4tDiVuQJepUu51e5eWYJpka624X4DMBxe3u\",\n",
        "    id_col=\"DATASET_ID\",\n",
        "    out_col=\"schema\",\n",
        "    max_retries=3, timeout=12, base_backoff=0.3,\n",
        "    sleep_between=0.0, verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "easP1PodtHiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_dataset_30d_arb[[\"DATASET_ID\", \"schema\"]])\n"
      ],
      "metadata": {
        "id": "nKbiELJn37HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dataset_30d_arb"
      ],
      "metadata": {
        "id": "0rX7qAT-NE7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOC-XmUpqCxL"
      },
      "source": [
        "## Section 2 Monitor hello world"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJIPh9iPj2AB"
      },
      "outputs": [],
      "source": [
        "\n",
        "req_plot(df_tasks_30d, v8_learn_debug, 40, \"V8_learn_debug\", \"iexec-hello-world-iapp-0.0.1\")\n",
        "\n",
        "req_count_with_detail(df_tasks_30d, v8_learn_debug, 40, \"V8_learn_debug\", \"iexec-hello-world-iapp-0.0.1\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NAhgbX6j2AB"
      },
      "source": [
        "## Section 3 Platform Availibility\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djiL7HGDCXkl"
      },
      "outputs": [],
      "source": [
        "# @title availibility plot\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming df_alltime_specific contains your DataFrame with the provided structure\n",
        "\n",
        "plt.figure(figsize=(20, 8))\n",
        "\n",
        "# Replace 'specific_workerpool_id' with the actual ID of the worker pool you're interested in\n",
        "specific_workerpool_id = workerpool_prod\n",
        "specific_app1 = 'docker.io/iexechub/python-hello-world:8.0.0-sconify-5.9.1-v15-production'\n",
        "specific_app2 = 'docker.io/iexechub/python-hello-world:8.0.0-sconify-5.7.6-v15-production'\n",
        "# Filter the DataFrame further for the specific worker pool\n",
        "df_alltime_specific1 = df_tasks_30d[df_tasks_30d['WORKERPOOL ID'] == specific_workerpool_id].copy()\n",
        "\n",
        "# Filter for specific applications\n",
        "df_alltime_app1 = df_alltime_specific1[df_alltime_specific1['APP MULTIADDR'] == specific_app1].copy()\n",
        "df_alltime_app2 = df_alltime_specific1[df_alltime_specific1['APP MULTIADDR'] == specific_app2].copy()\n",
        "\n",
        "# Combine the data for both applications\n",
        "df_alltime_specific = pd.concat([df_alltime_app1, df_alltime_app2])\n",
        "\n",
        "# Calculate the total minutes since midnight\n",
        "df_alltime_specific['Minutes'] = df_alltime_specific['DATE'].dt.hour * 60 + df_alltime_specific['DATE'].dt.minute\n",
        "\n",
        "# Apply modulo operation to get only hour and minutes\n",
        "df_alltime_specific['Hour_Minutes'] = df_alltime_specific['Minutes'] % (24 * 60)\n",
        "\n",
        "# Plot successful points\n",
        "plt.scatter(df_alltime_specific[df_alltime_specific['STATUS'] == 'COMPLETED']['Hour_Minutes'] / 60,\n",
        "            df_alltime_specific[df_alltime_specific['STATUS'] == 'COMPLETED']['DATE'],\n",
        "            color='green', label='Completed')\n",
        "\n",
        "# Plot unsuccessful points\n",
        "plt.scatter(df_alltime_specific[df_alltime_specific['STATUS'] != 'COMPLETED']['Hour_Minutes'] / 60,\n",
        "            df_alltime_specific[df_alltime_specific['STATUS'] != 'COMPLETED']['DATE'],\n",
        "            color='red', label='Other Statuses')\n",
        "\n",
        "plt.title('Black Box Test')\n",
        "plt.xlabel('Hours of the Day')\n",
        "plt.ylabel('Day')\n",
        "\n",
        "# Create a single legend for both success and failure\n",
        "plt.legend()\n",
        "\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQWK1ERdGKb2"
      },
      "outputs": [],
      "source": [
        "# @title availibility calculation\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming df_alltime_specific contains your DataFrame with the provided structure\n",
        "\n",
        "# Sort the DataFrame by DATE to ensure sequential observations\n",
        "df_alltime_specific_sorted = df_alltime_specific.sort_values(by='DATE')\n",
        "test=df_alltime_specific_sorted\n",
        "# Initialize variables to track \"up\" time intervals and downtime intervals\n",
        "up_intervals = []\n",
        "down_intervals = []\n",
        "\n",
        "# Initialize variables to track the start time and count of successful tasks within the interval\n",
        "start_time = None\n",
        "success_count = 0\n",
        "test.index = pd.RangeIndex(1, len(test.index) + 1)\n",
        "up=pd.Timedelta(0)\n",
        "down=pd.Timedelta(0)\n",
        "down_intervals = []\n",
        "\n",
        "# Iterate over the sorted DataFrame\n",
        "for index, row in test.iterrows():\n",
        "    if index < len(test) - 1:\n",
        "        next_row = test.loc[index + 1]\n",
        "\n",
        "        # Check if both the current and next observations are COMPLETED\n",
        "        if (row[\"STATUS\"] == \"COMPLETED\") and (next_row[\"STATUS\"] == \"COMPLETED\"):\n",
        "            # Check if the time between the current and next observations is less than or equal to 22 minutes\n",
        "            if (next_row['DATE'] - row[\"DATE\"]) <= pd.Timedelta(minutes=22):\n",
        "                up += next_row['DATE'] - row[\"DATE\"]\n",
        "            else:\n",
        "                # If the time between observations is more than 22 minutes, it's considered as down\n",
        "                down += next_row['DATE'] - row[\"DATE\"]\n",
        "                down_intervals.append((row['DATE'], next_row[\"DATE\"]))\n",
        "        else:\n",
        "            # If either the current or next observation is not COMPLETED, it's considered as down\n",
        "            down += next_row['DATE'] - row[\"DATE\"]\n",
        "            down_intervals.append((row['DATE'], next_row[\"DATE\"]))\n",
        "\n",
        "# Print down intervals\n",
        "print(\"Down Intervals:\")\n",
        "for start, end in down_intervals:\n",
        "    start_time = start.strftime('%Y-%m-%d %H:%M:%S')\n",
        "    end_time = end.strftime('%Y-%m-%d %H:%M:%S')\n",
        "    print(f\" [ {start_time}  -> {end_time} ] Duration: {end - start} \")\n",
        "\n",
        "# Merge consecutive intervals\n",
        "if len(down_intervals) > 0:\n",
        "    compressed_intervals = []\n",
        "    start, end = down_intervals[0]\n",
        "    for interval in down_intervals[1:]:\n",
        "        if interval[0] - end <= pd.Timedelta(minutes=1):\n",
        "            end = interval[1]\n",
        "        else:\n",
        "            compressed_intervals.append((start, end))\n",
        "            start, end = interval\n",
        "    compressed_intervals.append((start, end))\n",
        "\n",
        "    print(\"Down compressed:\")\n",
        "\n",
        "# Print compressed intervals\n",
        "print(\"\\nDown compressed Intervals:\")\n",
        "for start, end in compressed_intervals:\n",
        "    start_time = start.strftime('%Y-%m-%d %H:%M:%S')\n",
        "    end_time = end.strftime('%Y-%m-%d %H:%M:%S')\n",
        "    print(f\" [ {start_time}  -> {end_time} ] Duration: {end - start} \")\n",
        "\n",
        "# Calculate total time, availability percentage\n",
        "total_time = df_alltime_specific_sorted[\"DATE\"].max() - df_alltime_specific_sorted[\"DATE\"].min()\n",
        "up_percentage = (up / (up + down)) * 100\n",
        "\n",
        "# Print summary\n",
        "print(\"\\nSummary:\")\n",
        "print(f\"Total Time: {total_time}\")\n",
        "print(f\"Up period: {up}\")\n",
        "print(f\"Down period: {down}\")\n",
        "print(f\"Availability Percentage: {up_percentage}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHnLxrvPpYVS"
      },
      "source": [
        "## Section 4 : Visualize recent activity Bellecour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xP9lS59j2AC"
      },
      "outputs": [],
      "source": [
        "# @title Visualization per workerpool\n",
        "\n",
        "## V8-debug\n",
        "\n",
        "taskperday(df_tasks_30d, workerpool_debug,\"V8-debug\")\n",
        "\n",
        "taskplot(df_tasks_30d, workerpool_debug, 1, \"V8_debug\")\n",
        "taskplot(df_tasks_30d, workerpool_debug, 7, \"V8_debug\")\n",
        "taskplot(df_tasks_30d, workerpool_debug, 30, \"V8_debug\")\n",
        "\n",
        "successrate(df_tasks_30d, workerpool_debug, 1,  \"V8-debug\")\n",
        "successrate(df_tasks_30d, workerpool_debug, 7,  \"V8-debug\")\n",
        "successrate(df_tasks_30d, workerpool_debug, 30, \"V8-debug\")\n",
        "\n",
        "success_repartition(df_tasks_30d,workerpool_debug,1, \"V8_debug\",fixed_alpha=True)\n",
        "success_repartition(df_tasks_30d,workerpool_debug,7, \"V8_debug\",fixed_alpha=True)\n",
        "success_repartition(df_tasks_30d,workerpool_debug,30, \"V8_debug\",fixed_alpha=True)\n",
        "\n",
        "successrate_app(df_tasks_30d, workerpool_debug, \"V8-debug\")\n",
        "\n",
        "req_plot(df_tasks_30d, workerpool_debug, 30, \"V8_debug\", \"first-poc\")\n",
        "req_plot(df_tasks_30d, workerpool_debug, 30, \"V8_debug\", \"hello-world-scone\")\n",
        "\n",
        "## V8-prod\n",
        "\n",
        "taskperday(df_tasks_30d, workerpool_prod,\"V8-prod\")\n",
        "\n",
        "taskplot(df_tasks_30d, workerpool_prod, 1, \"V8_prod\")\n",
        "taskplot(df_tasks_30d, workerpool_prod, 7, \"V8_prod\")\n",
        "taskplot(df_tasks_30d, workerpool_prod, 35, \"V8_prod\")\n",
        "\n",
        "successrate(df_tasks_30d, workerpool_prod, 1,  \"V8-prod\")\n",
        "successrate(df_tasks_30d, workerpool_prod, 7,  \"V8-prod\")\n",
        "successrate(df_tasks_30d, workerpool_prod, 35, \"V8-prod\")\n",
        "\n",
        "success_repartition(df_tasks_30d,workerpool_prod,1, \"V8_prod\",fixed_alpha=True)\n",
        "success_repartition(df_tasks_30d,workerpool_prod,7, \"V8_prod\",fixed_alpha=True)\n",
        "success_repartition(df_tasks_30d,workerpool_prod,35, \"V8_prod\",fixed_alpha=True)\n",
        "\n",
        "successrate_app(df_tasks_30d, workerpool_prod, \"V8_prod\")\n",
        "\n",
        "#req_plot(df_tasks_30d, workerpool_prod, 35, \"V8_prod\", \"web3mail\")\n",
        "#req_plot(df_tasks_30d, workerpool_prod, 35, \"V8_prod\", \"generic-oracle-dapp\")\n",
        "\n",
        "#req_count(df_tasks_30d, workerpool_prod, 35, \"V8_prod\", \"web3mail\")\n",
        "#req_count(df_tasks_30d, workerpool_prod, 30, \"V8_prod\", \"generic-oracle-dapp\")\n",
        "\n",
        "## V8-learn-prod\n",
        "\n",
        "taskperday(df_tasks_30d,v8_learn_prod, \"V8-learn-prod\")\n",
        "\n",
        "taskplot(df_tasks_30d, v8_learn_prod, 1, \"V8-learn-prod\")\n",
        "taskplot(df_tasks_30d, v8_learn_prod, 7, \"V8-learn-prod\")\n",
        "taskplot(df_tasks_30d, v8_learn_prod, 35, \"V8-learn-prod\")\n",
        "\n",
        "successrate(df_tasks_30d, v8_learn_prod, 1,  \"v8_learn_prod\")\n",
        "successrate(df_tasks_30d, v8_learn_prod, 7,  \"v8_learn_prod\")\n",
        "successrate(df_tasks_30d, v8_learn_prod, 35, \"v8_learn_prod\")\n",
        "\n",
        "success_repartition(df_tasks_30d, v8_learn_prod,1, \"V8_learn_prod\",fixed_alpha=True)\n",
        "success_repartition(df_tasks_30d, v8_learn_prod,7, \"V8_learn_prod\",fixed_alpha=True)\n",
        "success_repartition(df_tasks_30d, v8_learn_prod,35, \"V8_learn_prod\",fixed_alpha=True)\n",
        "\n",
        "successrate_app(df_tasks_30d, v8_learn_prod, \"V8_learn_prod\")\n",
        "\n",
        "## V8-learn-debug\n",
        "\n",
        "taskperday(df_tasks_30d, v8_learn_debug, \"V8-learn-debug\")\n",
        "\n",
        "taskplot(df_tasks_30d, v8_learn_debug, 1, \"V8-learn-debug\")\n",
        "taskplot(df_tasks_30d, v8_learn_debug, 7, \"V8-learn-debug\")\n",
        "taskplot(df_tasks_30d, v8_learn_debug, 35, \"V8-learn-debug\")\n",
        "\n",
        "successrate(df_tasks_30d, v8_learn_debug, 1,  \"v8_learn_debug\")\n",
        "successrate(df_tasks_30d, v8_learn_debug, 7,  \"v8_learn_debug\")\n",
        "successrate(df_tasks_30d, v8_learn_debug, 35, \"v8_learn_debug\")\n",
        "\n",
        "success_repartition(df_tasks_30d, v8_learn_debug,1, \"V8_learn_debug\",fixed_alpha=True)\n",
        "success_repartition(df_tasks_30d, v8_learn_debug,7, \"V8_learn_debug\",fixed_alpha=True)\n",
        "success_repartition(df_tasks_30d, v8_learn_debug,35, \"V8_learn_debug\",fixed_alpha=True)\n",
        "\n",
        "successrate_app(df_tasks_30d, v8_learn_debug, \"V8_learn_debug\")\n",
        "\n",
        "#req_plot(df_tasks_30d, v8_learn_debug, 40, \"V8_learn_debug\", \"lejamon/tee_private_airdrop_claim\")\n",
        "#req_count(df_tasks_30d, v8_learn_debug, 40, \"V8_learn_debug\", \"lejamon/tee_private_airdrop_claim\")\n",
        "\n",
        "## wp_tdx\n",
        "\n",
        "taskperday(df_tasks_30d, wp_tdx, \"wp_tdx\")\n",
        "\n",
        "#taskplot(df_tasks_30d, wp_tdx, 1, \"wp_tdx\")\n",
        "#taskplot(df_tasks_30d, wp_tdx, 7, \"wp_tdx\")\n",
        "taskplot(df_tasks_30d, wp_tdx, 30, \"wp_tdx\")\n",
        "\n",
        "#successrate(df_tasks_30d, wp_tdx, 1,  \"wp_tdx\")\n",
        "#successrate(df_tasks_30d, wp_tdx, 7,  \"wp_tdx\")\n",
        "successrate(df_tasks_30d, wp_tdx, 30, \"wp_tdx\")\n",
        "\n",
        "#success_repartition(df_tasks_30d, wp_tdx,1, \"wp_tdx\", fixed_alpha=True)\n",
        "#success_repartition(df_tasks_30d, wp_tdx,7, \"wp_tdx\", fixed_alpha=True)\n",
        "success_repartition(df_tasks_30d, wp_tdx,35, \"wp_tdx\", fixed_alpha=True)\n",
        "\n",
        "successrate_app(df_tasks_30d, v8_learn_debug, \"wp_tdx\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dataset usage bellecour\n",
        "\n",
        "df_enriched = enrich_df_with_status(df_dataset_30d, endpoint=url_query_bellecour)\n"
      ],
      "metadata": {
        "id": "hqRAA2Te-GGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title print in tab dataset creation and usage\n",
        "tab_bellecour=dataset_usage(df_enriched, title=\"bellecour\")\n"
      ],
      "metadata": {
        "id": "DRrEQ3VH-gXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Top dataset owner on Bellecour\n",
        "top_owner_df_bellecour = map_addresses(\n",
        "    top_owner_df_bellecour,\n",
        "    columns=[\"Dataset Owner\"],\n",
        "    addr2name=addr2name\n",
        ")\n",
        "top_owner_df_bellecour = summarize_owners(tab_bellecour, top_n=50)\n",
        "\n",
        "\n",
        "print(tabulate(top_owner_df_bellecour, headers=\"keys\", tablefmt=\"github\"))\n"
      ],
      "metadata": {
        "id": "04zMOSFe-6WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Top dataset usage bellecour\n",
        "\n",
        "top_n_df_bellecour = top_datasets_by_usage(tab_bellecour, top_n=500)\n",
        "\n",
        "top_n_df_bellecour_named = map_addresses(\n",
        "    top_n_df_bellecour,\n",
        "    columns=[\"Dataset Owner\"],\n",
        "    addr2name=addr2name\n",
        ")\n",
        "\n",
        "print(tabulate(top_n_df_bellecour_named, headers='keys', tablefmt='github'))\n",
        "\n"
      ],
      "metadata": {
        "id": "02_cG_O2_OiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkWJIam9e9FF"
      },
      "outputs": [],
      "source": [
        "# @title Crosstab\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "# Assuming df is your DataFrame containing the data grouped by app name and worker pool ID\n",
        "crosstab = pd.crosstab(df_tasks_30d['APP NAME'], df_tasks_30d['WORKERPOOL ID'], margins=True, margins_name='Total')\n",
        "\n",
        "# Remove the 'Total' column and row\n",
        "crosstab = crosstab.drop('Total', axis=0)\n",
        "crosstab = crosstab.drop('Total', axis=1)\n",
        "\n",
        "# Calculate the total usage of each app across all worker pools\n",
        "app_total_usage = crosstab.sum(axis=1)\n",
        "\n",
        "# Sort the apps based on total usage\n",
        "sorted_apps = app_total_usage.sort_values(ascending=False).index\n",
        "\n",
        "# Reindex the crosstab DataFrame based on the sorted apps\n",
        "crosstab_sorted = crosstab.reindex(index=sorted_apps)\n",
        "\n",
        "# Create a heatmap using Seaborn\n",
        "plt.figure(figsize=(22, 22))\n",
        "heatmap = sns.heatmap(crosstab_sorted, annot=True, fmt='d', cmap='viridis')\n",
        "\n",
        "plt.title('APP vs workerpool')\n",
        "\n",
        "# Set xlabel on top\n",
        "plt.xlabel('WORKERPOOL ID', fontsize=14, labelpad=20, ha='center')\n",
        "\n",
        "heatmap.xaxis.tick_top()\n",
        "heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=90, ha='left')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_VYFRKjhIqa"
      },
      "outputs": [],
      "source": [
        "# @title Extract tasks in failure on the last 24 hours on V8-prod workerpool\n",
        "\n",
        "# Define the date range for the last days\n",
        "end_date = datetime.now(UTC)\n",
        "start_date = end_date - timedelta(days=1)  # Adjust the number of days as needed\n",
        "\n",
        "# Filter the DataFrame for the specified date range and workerpool ID\n",
        "filtered_df = df_tasks_30d[(df_tasks_30d['DATE'] >= start_date) & (df_tasks_30d['DATE'] <= end_date) & (df_tasks_30d['WORKERPOOL ID'] == workerpool_prod) & (df_tasks_30d['STATUS'] != 'COMPLETED')]\n",
        "\n",
        "# Exclude successful tasks\n",
        "filtered_df = filtered_df[filtered_df['STATUS'] != 'success']\n",
        "\n",
        "# Group by 'APP NAME' and aggregate Task IDs as a list\n",
        "unsuccessful_dapps_per_name = filtered_df.groupby('APP NAME')['TASK_ID'].agg(list)\n",
        "\n",
        "# Display the result\n",
        "for app_name, task_ids in unsuccessful_dapps_per_name.items():\n",
        "    print(f\"Unsuccessful DApp: {app_name}\")\n",
        "    for task_id in task_ids:\n",
        "        print(f\"Task ID: {task_id}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15nW7RrYHwB4"
      },
      "outputs": [],
      "source": [
        "# @title tops apps, wp, requesters\n",
        "\n",
        "\n",
        "# Assuming your dataset is stored in a DataFrame named df\n",
        "\n",
        "# Top 5 Apps\n",
        "top_apps = df_tasks_30d['APP NAME'].value_counts().nlargest(100)\n",
        "plt.figure(figsize=(12, 12))\n",
        "sns.barplot(y=top_apps.index, x=top_apps.values, palette='viridis')\n",
        "plt.title('Top 5 Apps')\n",
        "plt.xlabel('App Name')\n",
        "plt.ylabel('Frequency')\n",
        "#plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "# Top 5 Worker Pools\n",
        "top_workerpools = df_tasks_30d['WORKERPOOL ID'].value_counts().nlargest(100)\n",
        "plt.figure(figsize=(12,12))\n",
        "sns.barplot(y=top_workerpools.index, x=top_workerpools.values, palette='magma')\n",
        "plt.title('Top 5 Worker Pools')\n",
        "plt.xlabel('Worker Pool ID')\n",
        "plt.ylabel('Frequency')\n",
        "#plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "# Top 5 Requesters\n",
        "top_requesters = df_tasks_30d['REQUESTER ID'].value_counts().nlargest(5)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(y=top_requesters.index, x=top_requesters.values, palette='plasma')\n",
        "plt.title('Top 5 Requesters')\n",
        "plt.xlabel('Requester ID')\n",
        "plt.ylabel('Frequency')\n",
        "#plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rp0LiY-oiZ9d"
      },
      "outputs": [],
      "source": [
        "# @title Top apps in last days\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Filter the DataFrame for the last day\n",
        "duration=30\n",
        "end_date = datetime.now(UTC)\n",
        "start_date = end_date - timedelta(days=duration)\n",
        "df_datefiltered = df_tasks_30d[(df_tasks_30d['DATE'] >= start_date) & (df_tasks_30d['DATE'] <= end_date)]\n",
        "\n",
        "# Filter the DataFrame further for the specific worker pool\n",
        "df_wpfiltered = df_datefiltered[df_datefiltered['WORKERPOOL ID'] == workerpool_debug]\n",
        "\n",
        "# Group by 'APP NAME' and 'STATUS' and count occurrences\n",
        "app_counts = df_wpfiltered.groupby(['APP NAME', 'STATUS']).size()\n",
        "\n",
        "# Reset index to make 'APP NAME' and 'STATUS' as columns\n",
        "app_counts_df = app_counts.reset_index(name='Number of Occurrences')\n",
        "\n",
        "# Pivot the DataFrame to have 'STATUS' as columns\n",
        "app_counts_pivot = app_counts_df.pivot(index='APP NAME', columns='STATUS', values='Number of Occurrences')\n",
        "\n",
        "# Replace NaN values with zero\n",
        "app_counts_pivot.fillna(0, inplace=True)\n",
        "\n",
        "# Convert the DataFrame to use integer data type\n",
        "app_counts_pivot = app_counts_pivot.astype(int)\n",
        "\n",
        "# Add 'COMPLETED' column as sum of 'COMPLETED' and 'FAILED' columns\n",
        "app_counts_pivot['COMPLETED'] = app_counts_pivot['COMPLETED']\n",
        "\n",
        "# Sort the DataFrame based on 'COMPLETED' column\n",
        "app_counts_pivot_sorted = app_counts_pivot.sort_values(by='COMPLETED', ascending=False)\n",
        "\n",
        "# Print the DataFrame completely\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # Display all rows and columns\n",
        "    print(app_counts_pivot_sorted)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8tpQ8mh15Z-"
      },
      "source": [
        "## Section 4 bis: Visualize recent activity arbitrum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw8BIOzn15Z_"
      },
      "outputs": [],
      "source": [
        "# @title Visualization per workerpool\n",
        "\n",
        "## V8-debug_arb\n",
        "\n",
        "#taskperday(df_tasks_30d_arb, workerpool_debug_arb,\"V8-debug_arb\")\n",
        "\n",
        "taskplot(df_tasks_30d_arb, workerpool_debug_arb, 30, \"V8_debug_arb\")\n",
        "\n",
        "#successrate(df_tasks_30d_arb, workerpool_debug_arb, 30, \"V8-debug_arb\")\n",
        "\n",
        "success_repartition(df_tasks_30d_arb,workerpool_debug_arb,30, \"V8_debug_arb\",fixed_alpha=True)\n",
        "\n",
        "successrate_app(df_tasks_30d_arb, workerpool_debug_arb, \"V8-debug_arb\")\n",
        "\n",
        "\n",
        "## V8-prod_arb\n",
        "\n",
        "taskperday(df_tasks_30d_arb, workerpool_prod_arb,\"V8-prod_arb\")\n",
        "\n",
        "taskplot(df_tasks_30d_arb, workerpool_prod_arb, 30, \"V8_prod_arb\")\n",
        "\n",
        "successrate(df_tasks_30d_arb, workerpool_prod_arb, 30, \"V8-prod_arb\")\n",
        "\n",
        "success_repartition(df_tasks_30d_arb,workerpool_prod_arb,30, \"V8_prod_arb\",fixed_alpha=True)\n",
        "\n",
        "successrate_app(df_tasks_30d_arb, workerpool_prod_arb, \"V8-prod_arb\")\n",
        "\n",
        "req_count(df_tasks_30d_arb,workerpool_prod_arb, 30, \"V8-prod_arb\", \"web3mail\")\n",
        "req_count(df_tasks_30d_arb,workerpool_prod_arb, 30, \"V8-prod_arb\", \"web3telegram\")\n",
        "\n",
        "req_count(df_tasks_30d_arb,workerpool_prod_arb, 7, \"V8-prod_arb\", \"web3mail\")\n",
        "req_count(df_tasks_30d_arb,workerpool_prod_arb, 7, \"V8-prod_arb\", \"web3telegram\")\n",
        "\n",
        "#req_count(df_tasks_30d_arb,workerpool_prod_arb, 3, \"V8-prod_arb\", \"web3mail\")\n",
        "#req_count(df_tasks_30d_arb,workerpool_prod_arb, 3, \"V8-prod_arb\", \"web3telegram\")\n",
        "\n",
        "app_usage_per_day_tabulated(df_tasks_30d_arb, duration=35, workerpool_filter=workerpool_prod_arb, wp_name=\"V8-prod_arb\", top_k=6, hide_zero_days=True, include_total=True, tablefmt=\"psql\")   # nice boxed style)\n",
        "\n",
        "df_tasks_30d_arb= map_addresses(\n",
        "    df_tasks_30d_arb,\n",
        "   columns=[\"REQUESTER ID\"],\n",
        "    addr2name=addr2name\n",
        "    )\n",
        "\n",
        "app_usage_per_day_by_requester_tabulated(df_tasks_30d_arb, duration=35, workerpool_filter=workerpool_prod_arb, wp_name=\"V8-prod_arb\", top_requesters=None, top_k_per_app=6, hide_zero_days=True, include_total=True, tablefmt=\"psql\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_tasks_30d_arb.dtypes"
      ],
      "metadata": {
        "id": "CVtB57nxgfe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dataset usage arbitrum\n",
        "df_enriched_arb = enrich_df_with_status(df_dataset_30d_arb, endpoint=url_query_arbitrum)\n",
        "\n"
      ],
      "metadata": {
        "id": "n7ZI_gOP15aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dataset vs deal arbitrum\n",
        "\n",
        "scatter_deals_by_dataset_v3(df_enriched_arb,\n",
        "                  dataset_name_filter= None,\n",
        "                  duration=25)\n"
      ],
      "metadata": {
        "id": "9kdfd0Fa15aA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dataset usage on arbitrum\n",
        "\n",
        "tab_arbitrum=dataset_usage(df_enriched_arb, title=\"arbitrum\")"
      ],
      "metadata": {
        "id": "Ylde_sJC15aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "filename='/content/drive/Shareddrives/App_and_Product/dashboard_marketplace/data/database_donotusethisrepo/last_arbitrum_dataset.csv'\n",
        "tab_arbitrum.to_csv(filename, index=False)\n"
      ],
      "metadata": {
        "id": "lwNahK2PDlEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Top dataset owner on Arbitrum\n",
        "top_owner_df_arbitrum = summarize_owners(tab_arbitrum, top_n=50)\n",
        "print(tabulate(top_owner_df_arbitrum, headers=\"keys\", tablefmt=\"github\"))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yJYdDn6v15aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Top dataset usage arbitrum\n",
        "\n",
        "top_n_df_arbitrum = top_datasets_by_usage(tab_arbitrum, top_n=500)\n",
        "print(tabulate(top_n_df_arbitrum, headers='keys', tablefmt='github'))\n"
      ],
      "metadata": {
        "id": "ldvRexj915aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaZrS-L77nBl"
      },
      "source": [
        "## Section 5 : Save historical data in google drive\n",
        "\n",
        "Pre requisite : Google Drive connected\n",
        "Save data in google Drive, Control filename to avoid overwrite existing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSWigTVtB0hq"
      },
      "outputs": [],
      "source": [
        "# @title Save data Bellecour in csv file on google drive [USE CAREFULLY!!!!!!]\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Define the period\n",
        "\n",
        "#Method 1 with test\n",
        "date_begin = '2025-10-01 00:00:00'\n",
        "date_end = '2025-11-01 00:00:00'\n",
        "\n",
        "filename='/content/drive/Shareddrives/App_and_Product/dashboard_marketplace/data/database_donotusethisrepo/Dataset_2025_10.csv'\n",
        "date_begin = datetime.strptime(date_begin, '%Y-%m-%d %H:%M:%S').replace(tzinfo=UTC)\n",
        "date_end = datetime.strptime(date_end, '%Y-%m-%d %H:%M:%S').replace(tzinfo=UTC)\n",
        "\n",
        "print (\"Period to retrieve:\", date_begin, date_end)\n",
        "\n",
        "api_data = get_api_data_iexec_interval(url_query_bellecour, query, 0000, date_begin, date_end)\n",
        "df = pd.DataFrame(api_data, columns=[\"TASK_ID\",\"APP NAME\", \"APP MULTIADDR\", \"TAG\", \"STATUS\", \"DATE\", \"WORKERPOOL ID\", \"REQUESTER ID\", \"DATASET NAME\", \"DATASET_ID\"])\n",
        "df[\"DATE\"] = [datetime.utcfromtimestamp(int(x)) for x in df[\"DATE\"]]\n",
        "\n",
        "print(\"Period observed\", df[\"DATE\"].min(), df[\"DATE\"].max())\n",
        "print(\"dataset length: \" + str(df.shape[0]))\n",
        "\n",
        "#print(filename)\n",
        "#date_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "#new_filename = f\"{filename}.csv\"\n",
        "\n",
        "# Save the DataFrame to CSV with the new filename\n",
        "df.to_csv(filename, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Save data Arbitrum in csv file on google drive [USE CAREFULLY!!!!!!]\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Define the period\n",
        "\n",
        "#Method 1 with test\n",
        "date_begin = '2025-09-01 00:00:00'\n",
        "date_end = '2025-10-01 00:00:00'\n",
        "\n",
        "filename='/content/drive/Shareddrives/App_and_Product/dashboard_marketplace/data/database_donotusethisrepo/Arbitrum/Dataset_2025_09.csv'\n",
        "date_begin = datetime.strptime(date_begin, '%Y-%m-%d %H:%M:%S').replace(tzinfo=UTC)\n",
        "date_end = datetime.strptime(date_end, '%Y-%m-%d %H:%M:%S').replace(tzinfo=UTC)\n",
        "\n",
        "print (\"Period to retrieve:\", date_begin, date_end)\n",
        "\n",
        "api_data = get_api_data_iexec_interval(url_query_arbitrum, query, 0000, date_begin, date_end)\n",
        "df = pd.DataFrame(api_data, columns=[\"TASK_ID\",\"APP NAME\", \"APP MULTIADDR\", \"TAG\", \"STATUS\", \"DATE\", \"WORKERPOOL ID\", \"REQUESTER ID\", \"DATASET NAME\", \"DATASET_ID\"])\n",
        "df[\"DATE\"] = [datetime.utcfromtimestamp(int(x)) for x in df[\"DATE\"]]\n",
        "\n",
        "print(\"Period observed\", df[\"DATE\"].min(), df[\"DATE\"].max())\n",
        "print(\"dataset length: \" + str(df.shape[0]))\n",
        "\n",
        "#print(filename)\n",
        "#date_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "#new_filename = f\"{filename}.csv\"\n",
        "\n",
        "# Save the DataFrame to CSV with the new filename\n",
        "df.to_csv(filename, index=False)"
      ],
      "metadata": {
        "id": "_MglYf3OLuFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqaf-dV0TZBK"
      },
      "source": [
        "## Section 6 : Load full historical data from google drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnAP4a38BsvA"
      },
      "outputs": [],
      "source": [
        "# @title Load dataset from csv file store on google drive\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Directory containing CSV files\n",
        "directory='/content/drive/Shareddrives/App_and_Product/dashboard_marketplace/data/database_donotusethisrepo/Arbitrum'\n",
        "\n",
        "# List to store DataFrames from each CSV file\n",
        "dfs = []\n",
        "\n",
        "# Iterate over each file in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.csv'):\n",
        "        # Read CSV file into a DataFrame\n",
        "        filepath = os.path.join(directory, filename)\n",
        "        dftmp = pd.read_csv(filepath)\n",
        "        dftmp['DATE'] = pd.to_datetime(dftmp['DATE'])\n",
        "        _begin = dftmp['DATE'].min().strftime(\"%Y-%m-%d\")\n",
        "        _end = dftmp['DATE'].max().strftime(\"%Y-%m-%d\")\n",
        "        print(str(filename) + \" \" + str(_begin) + \" \" + str(_end))\n",
        "        # Append DataFrame to the list\n",
        "        dfs.append(dftmp)\n",
        "\n",
        "# Concatenate all DataFrames into a single DataFrame\n",
        "concatenated_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "\n",
        "_begin = concatenated_df['DATE'].min().strftime(\"%Y-%m-%d\")\n",
        "_end = concatenated_df['DATE'].max().strftime(\"%Y-%m-%d\")\n",
        "print(\"global dataframe: \" + str(concatenated_df.shape) +\" \" + str(_begin) + \" \" + str(_end))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQyjLpVMcqB8"
      },
      "outputs": [],
      "source": [
        "# @title show header\n",
        "\n",
        "concatenated_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juyRg4dka1PD"
      },
      "outputs": [],
      "source": [
        "# @title Display dataset struture and date info\n",
        "\n",
        "print(\"Period observed\", concatenated_df[\"DATE\"].min(), concatenated_df[\"DATE\"].max())\n",
        "\n",
        "print(\"dataset length: \" + str(concatenated_df.shape[0]))\n",
        "\n",
        "concatenated_df.dtypes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYZVEqeSBvEn"
      },
      "source": [
        "## Section 7 : Visualize full historical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lPOy13zBtix"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28aaXaqV3BMZ"
      },
      "outputs": [],
      "source": [
        "# @title Monthly distribution\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the date interval\n",
        "date_begin = concatenated_df['DATE'].min().strftime(\"%Y-%m-%d\")\n",
        "date_end = concatenated_df['DATE'].max().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# Extract the hour component from the 'DATE' column\n",
        "concatenated_df['Hour'] = concatenated_df['DATE'].dt.hour\n",
        "\n",
        "# Calculate the total number of tasks for each hour of the day\n",
        "task_counts = concatenated_df.groupby('Hour').size()\n",
        "\n",
        "# Calculate the success ratio for each hour of the day\n",
        "success_counts = concatenated_df[concatenated_df['STATUS'] == 'COMPLETED'].groupby('Hour').size()\n",
        "success_ratio = success_counts / task_counts\n",
        "\n",
        "# Create a figure with two subplots\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Plot the distribution of tasks for each hour of the day\n",
        "ax1.hist(concatenated_df['Hour'], bins=24, color='skyblue', edgecolor='black', alpha=0.7, label='Task Distribution')\n",
        "ax1.set_xlabel('Hour of the Day')\n",
        "ax1.set_ylabel('Number of Tasks')\n",
        "ax1.set_title(f'Distribution of Tasks and Success Ratio for Each Hour of the Day \\n ({date_begin} to {date_end})')\n",
        "ax1.set_xticks(range(24))\n",
        "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "ax1.legend(loc='upper left')\n",
        "\n",
        "# Create a secondary y-axis for success ratio\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(range(24), success_ratio, color='orange', marker='o', label='Success Ratio')\n",
        "ax2.set_ylabel('Success Ratio')\n",
        "ax2.set_ylim(0, 1)\n",
        "ax2.grid(False)\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSLTYWG0HuUr"
      },
      "outputs": [],
      "source": [
        "# @title Distribution of Tasks and Success Ratio for Each Hour of the Day\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the date interval\n",
        "date_begin = concatenated_df['DATE'].min().strftime(\"%Y-%m-%d\")\n",
        "date_end = concatenated_df['DATE'].max().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# Filter the DataFrame to include only tasks associated with the specified workerpool\n",
        "workerpool_df = concatenated_df[concatenated_df['WORKERPOOL ID'] == workerpool_prod]  # Replace '0X7...' with the actual workerpool ID\n",
        "\n",
        "# Extract the hour component from the 'DATE' column\n",
        "workerpool_df['Hour'] = workerpool_df['DATE'].dt.hour\n",
        "\n",
        "# Calculate the total number of tasks for each hour of the day within the specified workerpool\n",
        "task_counts = workerpool_df.groupby('Hour').size()\n",
        "\n",
        "# Calculate the success ratio for each hour of the day within the specified workerpool\n",
        "success_counts = workerpool_df[workerpool_df['STATUS'] == 'COMPLETED'].groupby('Hour').size()\n",
        "success_ratio = success_counts / task_counts\n",
        "\n",
        "# Create a figure with two subplots\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Plot the distribution of tasks for each hour of the day within the specified workerpool\n",
        "ax1.hist(workerpool_df['Hour'], bins=24, color='skyblue', edgecolor='black', alpha=0.7, label='Task Distribution')\n",
        "ax1.set_xlabel('Hour of the Day')\n",
        "ax1.set_ylabel('Number of Tasks')\n",
        "ax1.set_title(f'Distribution of Tasks and Success Ratio for Each Hour of the Day \\n ({date_begin} to {date_end}) - Workerpool: 0X7...')\n",
        "ax1.set_xticks(range(24))\n",
        "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "ax1.legend(loc='upper left')\n",
        "\n",
        "# Create a secondary y-axis for success ratio\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(range(24), success_ratio, color='orange', marker='o', label='Success Ratio')\n",
        "ax2.set_ylabel('Success Ratio')\n",
        "ax2.set_ylim(0, 1)\n",
        "ax2.grid(False)\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCapvgB4JqFf"
      },
      "outputs": [],
      "source": [
        "# @title Distribution of Tasks and Success Ratio for Each Day of the Week\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the date interval\n",
        "date_begin = concatenated_df['DATE'].min().strftime(\"%Y-%m-%d\")\n",
        "date_end = concatenated_df['DATE'].max().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# Filter the DataFrame to include only tasks associated with the specified workerpool\n",
        "workerpool_df = concatenated_df[concatenated_df['WORKERPOOL ID'] == workerpool_prod]  # Replace '0X7...' with the actual workerpool ID\n",
        "\n",
        "# Extract the day of the week component from the 'DATE' column\n",
        "workerpool_df['Day of Week'] = workerpool_df['DATE'].dt.dayofweek\n",
        "\n",
        "# Calculate the total number of tasks for each day of the week within the specified workerpool\n",
        "task_counts = workerpool_df.groupby('Day of Week').size()\n",
        "\n",
        "# Calculate the success ratio for each day of the week within the specified workerpool\n",
        "success_counts = workerpool_df[workerpool_df['STATUS'] == 'COMPLETED'].groupby('Day of Week').size()\n",
        "success_ratio = success_counts / task_counts\n",
        "\n",
        "# Create a figure with two subplots\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Plot the distribution of tasks for each day of the week within the specified workerpool\n",
        "ax1.bar(range(7), task_counts, color='skyblue', label='Task Distribution')\n",
        "ax1.set_xlabel('Day of the Week')\n",
        "ax1.set_ylabel('Number of Tasks')\n",
        "ax1.set_title(f'Distribution of Tasks and Success Ratio for Each Day of the Week \\n ({date_begin} to {date_end}) - Workerpool: {workerpool_prod}')\n",
        "ax1.set_xticks(range(7))\n",
        "ax1.set_xticklabels(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
        "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "ax1.legend(loc='upper left')\n",
        "\n",
        "# Create a secondary y-axis for success ratio\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(range(7), success_ratio, color='orange', marker='o', label='Success Ratio')\n",
        "ax2.set_ylabel('Success Ratio')\n",
        "ax2.set_ylim(0, 1)\n",
        "ax2.grid(False)\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gZdSRfbs19m"
      },
      "outputs": [],
      "source": [
        "# @title Worker Pool ID Usage\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure 'DATE' column is in datetime format\n",
        "concatenated_df['DATE'] = pd.to_datetime(concatenated_df['DATE'])\n",
        "\n",
        "# Extract month and year from the 'DATE' column\n",
        "concatenated_df['MONTH'] = concatenated_df['DATE'].dt.month\n",
        "concatenated_df['YEAR'] = concatenated_df['DATE'].dt.year\n",
        "\n",
        "# Group by 'WORKERPOOL ID', 'YEAR', and 'MONTH', and count the occurrences\n",
        "usage_per_month = concatenated_df.groupby(['WORKERPOOL ID', 'YEAR', 'MONTH']).size().reset_index(name='COUNT')\n",
        "\n",
        "# Aggregate the counts of all worker pool IDs\n",
        "total_counts = usage_per_month.groupby('WORKERPOOL ID')['COUNT'].sum()\n",
        "\n",
        "# Select the top 15 worker pool IDs\n",
        "top_15_workerpool = total_counts.nlargest(15)\n",
        "\n",
        "# Group the rest into a new label \"Others\"\n",
        "other_count = total_counts.drop(top_15_workerpool.index).sum()\n",
        "\n",
        "# Combine the counts of the top 15 IDs and the rest\n",
        "combined_counts = pd.concat([top_15_workerpool, pd.Series({'Others': other_count})])\n",
        "\n",
        "# Plot the worker pool ID distribution usage grouped per month\n",
        "plt.figure(figsize=(12, 8))\n",
        "combined_counts.plot(kind='bar', color='blue', alpha=0.7)\n",
        "\n",
        "# Add labels outside the plot\n",
        "for i, (label, count) in enumerate(combined_counts.items()):\n",
        "    plt.text(i, count + 100, str(count), ha='center', va='bottom')\n",
        "\n",
        "plt.title('Worker Pool ID Usage Grouped Per Month (Top 15)')\n",
        "plt.xlabel('Worker Pool ID')\n",
        "plt.ylabel('Number of Occurrences')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rL9WJQZtJfqV"
      },
      "outputs": [],
      "source": [
        "# @title Activity on 4 main workerpools\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Assuming your DataFrame is named 'df'\n",
        "# Convert DATE column to datetime if it's not already\n",
        "concatenated_df['DATE'] = pd.to_datetime(concatenated_df['DATE'])\n",
        "tmp_df = concatenated_df[\n",
        "    (concatenated_df['WORKERPOOL ID'] == workerpool_prod) |\n",
        "    (concatenated_df['WORKERPOOL ID'] == workerpool_debug) |\n",
        "    (concatenated_df['WORKERPOOL ID'] == v8_learn_debug) |\n",
        "    (concatenated_df['WORKERPOOL ID'] == v8_learn_prod)\n",
        "]\n",
        "# Extract month from the DATE column\n",
        "tmp_df['Month'] = tmp_df['DATE'].dt.to_period('M')\n",
        "\n",
        "# Group by Month and STATUS, count occurrences\n",
        "status_counts = tmp_df.groupby(['Month', 'STATUS']).size().unstack(fill_value=0)\n",
        "\n",
        "# Plot\n",
        "status_counts.plot(kind='bar', stacked=True, figsize=(20, 8))\n",
        "plt.title('Distribution of Task (with status) reached per Month- ALL 4 workerpools')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Count (log scale)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Status')\n",
        "\n",
        "# Change y-axis to log scale\n",
        "# plt.yscale('log')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNNtdOAbziuO"
      },
      "outputs": [],
      "source": [
        "# @title Month activity on 4 main workerpools\n",
        "\n",
        "\n",
        "def month_plot(df, workerpool_filter, start, end, wp_name):\n",
        "    date_begin = datetime.strptime(start, '%Y-%m-%d %H:%M:%S')\n",
        "    date_end = datetime.strptime(end, '%Y-%m-%d %H:%M:%S')\n",
        "    # Convert start and end dates to datetime if they are not already\n",
        "    if isinstance(start, str):\n",
        "        start = datetime.strptime(start, '%Y-%m-%d %H:%M:%S')\n",
        "    if isinstance(end, str):\n",
        "        end = datetime.strptime(end, '%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    # Filter the DataFrame with date start, end\n",
        "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
        "    df_filtered = df[(df['DATE'] >= start) & (df['DATE'] <= end)]\n",
        "\n",
        "    # Filter the DataFrame further for the specific worker pool\n",
        "    df_prodhist = df_filtered[df_filtered['WORKERPOOL ID'] == workerpool_filter]\n",
        "\n",
        "    # Debug: Check workerpool filtered DataFrame\n",
        "    # Extract month from the DATE column\n",
        "    df_prodhist['Month'] = df_prodhist['DATE'].dt.to_period('M')\n",
        "\n",
        "    # Group by Month and STATUS, count occurrences\n",
        "    status_counts = df_prodhist.groupby(['Month', 'STATUS']).size().unstack(fill_value=0)\n",
        "\n",
        "    # Plot\n",
        "    if not status_counts.empty:\n",
        "        status_counts.plot(kind='bar', stacked=True, figsize=(20, 8))\n",
        "#      plt.title(f'Distribution of Task (with status) reached per Month - {wp_name}')\n",
        "        plt.title('Distribution of Task (with status) reached per Month \\n Workerpool: {} \\n Date : {} -> {}'.format(wp_name, start, end))\n",
        "        plt.xlabel('Month')\n",
        "        plt.ylabel('Count')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.legend(title='Status')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"[DEBUG] No numeric data to plot.\")\n",
        "\n",
        "# Example usage\n",
        "\n",
        "month_plot(concatenated_df, workerpool_prod,'2023-01-01 00:00:00','2025-09-01 00:00:00', \"Workerpool Prod\")\n",
        "month_plot(concatenated_df, workerpool_debug,'2023-01-01 00:00:00','2025-09-01 00:00:00', \"Workerpool debug\")\n",
        "month_plot(concatenated_df, v8_learn_debug,'2024-07-01 00:00:00','2025-09-01 00:00:00', \"v8_learn_debug\")\n",
        "month_plot(concatenated_df, v8_learn_prod,'2024-07-01 00:00:00','2025-09-01 00:00:00', \"v8_learn_prod\")\n",
        "\n",
        "\n",
        "def combined_workerpool_plot(df, workerpool_configs, start, end):\n",
        "    \"\"\"\n",
        "    Create a combined plot for multiple workerpools showing task distribution over time.\n",
        "    All workerpools will use the same date range for comparison.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with the data\n",
        "        workerpool_configs: List of tuples containing (workerpool_id, name)\n",
        "        start: Global start date for all workerpools\n",
        "        end: Global end date for all workerpools\n",
        "    \"\"\"\n",
        "    # Convert dates to datetime\n",
        "    date_start = datetime.strptime(start, '%Y-%m-%d %H:%M:%S')\n",
        "    date_end = datetime.strptime(end, '%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    # Convert DataFrame dates\n",
        "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
        "\n",
        "    # Create figure with subplots\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Color palette for different workerpools\n",
        "    colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\n",
        "\n",
        "    # Filter the main DataFrame for the date range\n",
        "    df_date_filtered = df[(df['DATE'] >= date_start) & (df['DATE'] <= date_end)]\n",
        "\n",
        "    # Store data for plotting\n",
        "    all_monthly_counts = []\n",
        "\n",
        "    for idx, (workerpool_id, name) in enumerate(workerpool_configs):\n",
        "        # Filter for this workerpool\n",
        "        df_filtered = df_date_filtered[df_date_filtered['WORKERPOOL ID'] == workerpool_id]\n",
        "\n",
        "        # Get monthly counts\n",
        "        df_filtered['Month'] = df_filtered['DATE'].dt.to_period('M')\n",
        "        monthly_counts = df_filtered.groupby('Month').size()\n",
        "\n",
        "        # Ensure all months are present (fill missing months with 0)\n",
        "        date_range = pd.period_range(start=date_start, end=date_end, freq='M')\n",
        "        monthly_counts = monthly_counts.reindex(date_range, fill_value=0)\n",
        "\n",
        "        # Store data for plotting\n",
        "        all_monthly_counts.append((monthly_counts, name, colors[idx]))\n",
        "\n",
        "    # Plot all workerpools\n",
        "    bar_width = 0.2\n",
        "    for idx, (counts, name, color) in enumerate(all_monthly_counts):\n",
        "        positions = [i + (idx * bar_width) for i in range(len(counts))]\n",
        "        plt.bar(positions, counts.values, bar_width,\n",
        "                label=name, color=color, alpha=0.7)\n",
        "\n",
        "    # Customize plot\n",
        "    plt.title('Combined Task Distribution Across Workerpools\\n'\n",
        "              f'Period: {start} -> {end}', pad=20)\n",
        "    plt.xlabel('Month')\n",
        "    plt.ylabel('Number of Tasks')\n",
        "\n",
        "    # Set x-axis labels\n",
        "    if all_monthly_counts:\n",
        "        months = all_monthly_counts[0][0].index\n",
        "        plt.xticks([i + (bar_width * (len(workerpool_configs)-1)/2) for i in range(len(months))],\n",
        "                  months.astype(str),\n",
        "                  rotation=45)\n",
        "\n",
        "    plt.legend(title='Workerpool', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return plt.gcf()\n",
        "\n",
        "# Example usage\n",
        "workerpool_configs = [\n",
        "    (workerpool_prod, \"Workerpool Prod\"),\n",
        "    (workerpool_debug, \"Workerpool Debug\"),\n",
        "    (v8_learn_debug, \"v8_learn_debug\"),\n",
        "    (v8_learn_prod, \"v8_learn_prod\")\n",
        "]\n",
        "\n",
        "fig = combined_workerpool_plot(concatenated_df, workerpool_configs,\n",
        "                             '2024-01-01 00:00:00', '2025-09-01 00:00:00')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIXYpa4Fj2AD"
      },
      "outputs": [],
      "source": [
        "# @title App table on 4 main workerpool\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def month_table(df, workerpool_filter, start, end, wp_name, filename=None):\n",
        "    # Filter the DataFrame with date start, end\n",
        "    df_filtered = df[(df['DATE'] >= start) & (df['DATE'] <= end)]\n",
        "\n",
        "    # Filter the DataFrame further for the specific worker pool\n",
        "    df_filtered = df_filtered[df_filtered['WORKERPOOL ID'] == workerpool_filter]\n",
        "\n",
        "    # Extract month from the DATE column\n",
        "    df_filtered['Month'] = df_filtered['DATE'].dt.to_period('M')\n",
        "\n",
        "    # Group by Month and APP NAME, count occurrences\n",
        "    app_counts = df_filtered.groupby(['Month', 'APP NAME']).size().unstack(fill_value=0)\n",
        "\n",
        "    # Add a \"Total\" column to calculate the sum of each row\n",
        "    app_counts['Total'] = app_counts.sum(axis=1)\n",
        "\n",
        "    # Transpose the DataFrame to reverse lines and columns\n",
        "    app_counts_sorted_transposed = app_counts.T\n",
        "\n",
        "    # Add a column for the total for each line\n",
        "    app_counts_sorted_transposed['Total'] = app_counts_sorted_transposed.sum(axis=1)\n",
        "\n",
        "    # Sort the DataFrame by the \"Total\" column\n",
        "    app_counts_sorted_transposed = app_counts_sorted_transposed.sort_values(by='Total', ascending=False)\n",
        "\n",
        "    # Format the transposed DataFrame as a table\n",
        "    csvtable = app_counts_sorted_transposed\n",
        "    table = app_counts_sorted_transposed.to_string()\n",
        "\n",
        "    # Display the table\n",
        "    print('App usage per month on Workerpool: {} {} \\nPeriod: [{} -> {}]\\n'.format(wp_name, workerpool_filter, start, end))\n",
        "    print(table)\n",
        "\n",
        "    # Save the table to a CSV file if filename is provided\n",
        "    if filename:\n",
        "        app_counts_sorted_transposed.to_csv(filename)\n",
        "        print(f'Table saved to {filename}')\n",
        "\n",
        "    return csvtable\n",
        "\n",
        "\n",
        "# Example usage\n",
        "summary_prod=month_table(concatenated_df, workerpool_prod, '2024-09-01 00:00:00', '2025-05-01 00:00:00', \"Workerpool Prod\")\n",
        "summary_debug=month_table(concatenated_df, workerpool_debug, '2024-09-01 00:00:00', '2025-05-01 00:00:00', \"Workerpool Debug\")\n",
        "summary_learn_prod=month_table(concatenated_df, v8_learn_prod, '2024-09-01 00:00:00', '2025-05-01 00:00:00', \"v8_learn_prod\")\n",
        "summary_learn_debug=month_table(concatenated_df, v8_learn_debug, '2024-09-01 00:00:00', '2025-05-01 00:00:00', \"v8_learn_debug\")\n",
        "\n",
        "#month_table(concatenated_df, workerpool_prod, '2023-01-01 00:00:00', '2024-11-01 00:00:00', \"V8 prod\", filename=\"/content/drive/Shareddrives/App_and_Product/dashboard_marketplace/V8-prod_Jan23-may24.csv\")\n",
        "#month_table(concatenated_df, workerpool_debug, '2023-01-01 00:00:00', '2024-11-01 00:00:00', \"V8 debug\",filename=\"/content/drive/Shareddrives/App_and_Product/dashboard_marketplace/V8-debug_Jan23-may24.csv\")\n",
        "\n",
        "#summary_prod.to_csv(\"/content/drive/Shareddrives/App_and_Product/dashboard_marketplace/summary-wp-prod.csv\")\n",
        "#summary_debug.to_csv(\"/content/drive/Shareddrives/App_and_Product/dashboard_marketplace/summary-wp-debug.csv\")\n",
        "#summary_learn_prod.to_csv(\"/content/drive/Shareddrives/App_and_Product/dashboard_marketplace/summary-wp-learn-prod.csv\")\n",
        "#summary_learn_debug.to_csv(\"/content/drive/Shareddrives/App_and_Product/dashboard_marketplace/summary-wp-learn-debug.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvH98ftoj2AH"
      },
      "outputs": [],
      "source": [
        "# @title table saving\n",
        "\n",
        "def save_period(df, start, end, filename):\n",
        "    date_begin = datetime.strptime(start, '%Y-%m-%d %H:%M:%S')\n",
        "    date_end = datetime.strptime(end, '%Y-%m-%d %H:%M:%S')\n",
        "    # Convert start and end dates to datetime if they are not already\n",
        "    if isinstance(start, str):\n",
        "        start = datetime.strptime(start, '%Y-%m-%d %H:%M:%S')\n",
        "    if isinstance(end, str):\n",
        "        end = datetime.strptime(end, '%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    # Filter the DataFrame with date start, end\n",
        "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
        "    df_filtered = df[(df['DATE'] >= start) & (df['DATE'] <= end)]\n",
        "\n",
        "\n",
        "    print(\"Period observed\", df_filtered[\"DATE\"].min(), df_filtered[\"DATE\"].max())\n",
        "    print(\"dataset length: \" + str(df_filtered.shape[0]))\n",
        "\n",
        "\n",
        "    # Save the DataFrame to CSV with the new filename !!! uncomment\n",
        "    #df_filtered.to_csv(filename, index=False)\n",
        "\n",
        "\n",
        "#filename='/content/drive/Shareddrives/App_and_Product/dashboard_marketplace/data/tasks_2023.csv'\n",
        "#save_period(concatenated_df,'2023-01-01 00:00:00','2024-01-01 00:00:00',filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsYdvaUq1DdJ"
      },
      "outputs": [],
      "source": [
        "# @title Apps success ratio [only v8-prod]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter the DataFrame further for the specific worker pool\n",
        "df_prodhist = concatenated_df[concatenated_df['WORKERPOOL ID'] == workerpool_prod]\n",
        "# Calculate the success ratio for each DApp\n",
        "dapp_data = {}\n",
        "for dapp, group in df_prodhist.groupby('APP NAME'):\n",
        "    total_count = group.shape[0]\n",
        "    success_count = group[group['STATUS'] == 'COMPLETED'].shape[0]\n",
        "    ratio = success_count / total_count * 100 if total_count != 0 else 0\n",
        "    dapp_data[dapp] = {'ratio': ratio, 'success_count': success_count, 'total_count': total_count}\n",
        "\n",
        "# Sort dapp_data by total_count\n",
        "sorted_dapp_data = sorted(dapp_data.items(), key=lambda x: x[1]['total_count'], reverse=False)\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(20, 20))\n",
        "plt.barh([dapp[0] for dapp in sorted_dapp_data], [d[1]['ratio'] for d in sorted_dapp_data], color='lightgrey', label='Success Ratio')\n",
        "plt.xlabel('Success Ratio in %', fontsize=20)  # Set larger fontsize for xlabel\n",
        "plt.ylabel('DApp', fontsize=20)  # Set larger fontsize for ylabel\n",
        "plt.title(f'Success Ratio of Tasks for Each DApp on V8-prod({date_begin} to {date_end})', fontsize=24)  # Set larger fontsize for title\n",
        "plt.legend()\n",
        "\n",
        "# Set larger fontsize for tick labels\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "\n",
        "# Add annotations for success ratio and total task count\n",
        "for i, (dapp, data) in enumerate(sorted_dapp_data):\n",
        "    plt.text(3 , i, f'{data[\"ratio\"]:.2f}% ({data[\"success_count\"]} out of {data[\"total_count\"]} tasks)', verticalalignment='center', fontsize=14)  # Set larger fontsize for annotations\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the plot before displaying it\n",
        "output_path = \"/content/drive/Shareddrives/App_and_Product/dashboard_marketplace/Dashboard_from_collab\"\n",
        "#filename = output_path + \"/Dapp_Success_rate_on_V8-prod-alltime.jpg\"\n",
        "#plt.savefig(filename)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9KPLT_vj2AH"
      },
      "outputs": [],
      "source": [
        "# @title Availibility history\n",
        "\n",
        "\n",
        "# Assuming df_alltime_specific contains your DataFrame with the provided structure\n",
        "\n",
        "def availability(df, workerpool_filter, start, end):\n",
        "    # Filter the DataFrame further for the specific worker pool\n",
        "    df_filtered = df[df['WORKERPOOL ID'] == workerpool_filter]\n",
        "    df_filtered = df_filtered.sort_values(by='DATE')\n",
        "    specific_app1 = 'docker.io/iexechub/python-hello-world:8.0.0-sconify-5.7.5-v14-production'\n",
        "    specific_app2 = 'docker.io/iexechub/python-hello-world:8.0.0-sconify-5.7.6-v15-production'\n",
        "    specific_app3 = 'docker.io/iexechub/python-hello-world:8.0.0-sconify-5.9.1-v15-production'\n",
        "\n",
        "    # Filter for specific applications\n",
        "    df_filtered = df_filtered[(df_filtered['APP MULTIADDR'] == specific_app1) | (df_filtered['APP MULTIADDR'] == specific_app2) | (df_filtered['APP MULTIADDR'] == specific_app3)].copy()\n",
        "\n",
        "    # Filter the DataFrame with date start, end\n",
        "    df_filtered = df_filtered[(df_filtered['DATE'] >= start) & (df_filtered['DATE'] <= end)]\n",
        "\n",
        "    test = df_filtered\n",
        "    # Initialize variables to track \"up\" time intervals and downtime intervals\n",
        "    up_intervals = []\n",
        "    down_intervals = []\n",
        "\n",
        "    # Initialize variables to track the start time and count of successful tasks within the interval\n",
        "    start_time = None\n",
        "    success_count = 0\n",
        "    test.index = pd.RangeIndex(1, len(test.index) + 1)\n",
        "    up = pd.Timedelta(0)\n",
        "    down = pd.Timedelta(0)\n",
        "    down_intervals = []\n",
        "\n",
        "    # Iterate over the sorted DataFrame\n",
        "    for index, row in test.iterrows():\n",
        "        if index < len(test) - 1:\n",
        "            next_row = test.loc[index + 1]\n",
        "\n",
        "            # Check if both the current and next observations are COMPLETED\n",
        "            if (row[\"STATUS\"] == \"COMPLETED\") and (next_row[\"STATUS\"] == \"COMPLETED\"):\n",
        "                # Check if the time between the current and next observations is less than or equal to 22 minutes\n",
        "                if (next_row['DATE'] - row[\"DATE\"]) <= pd.Timedelta(minutes=22):\n",
        "                    up += next_row['DATE'] - row[\"DATE\"]\n",
        "                else:\n",
        "                    # If the time between observations is more than 22 minutes, it's considered as down\n",
        "                    down += next_row['DATE'] - row[\"DATE\"]\n",
        "                    down_intervals.append((row['DATE'], next_row[\"DATE\"]))\n",
        "            else:\n",
        "                # If either the current or next observation is not COMPLETED, it's considered as down\n",
        "                down += next_row['DATE'] - row[\"DATE\"]\n",
        "                down_intervals.append((row['DATE'], next_row[\"DATE\"]))\n",
        "\n",
        "    # Print down intervals\n",
        "    print(\"Down Intervals:\")\n",
        "    for start, end in down_intervals:\n",
        "        start_time = start.strftime('%Y-%m-%d %H:%M:%S')\n",
        "        end_time = end.strftime('%Y-%m-%d %H:%M:%S')\n",
        "        print(f\" [ {start_time}  -> {end_time} ] Duration: {end - start} \")\n",
        "\n",
        "    # Merge consecutive intervals\n",
        "    compressed_intervals = []\n",
        "\n",
        "    # Check if down_intervals is not empty before processing\n",
        "    if down_intervals:\n",
        "        start, end = down_intervals[0]\n",
        "        for interval in down_intervals[1:]:\n",
        "            if interval[0] - end <= pd.Timedelta(minutes=1):\n",
        "                end = interval[1]\n",
        "            else:\n",
        "                compressed_intervals.append((start, end))\n",
        "                start, end = interval\n",
        "        compressed_intervals.append((start, end))\n",
        "\n",
        "        print(\"Down compressed:\")\n",
        "\n",
        "        # Print compressed intervals\n",
        "        print(\"\\nDown compressed Intervals:\")\n",
        "        for start, end in compressed_intervals:\n",
        "            start_time = start.strftime('%Y-%m-%d %H:%M:%S')\n",
        "            end_time = end.strftime('%Y-%m-%d %H:%M:%S')\n",
        "            print(f\" [ {start_time}  -> {end_time} ] Duration: {end - start} \")\n",
        "    else:\n",
        "        print(\"No down intervals to compress\")\n",
        "\n",
        "    # Calculate total time, availability percentage\n",
        "    total_time = df_filtered[\"DATE\"].max() - df_filtered[\"DATE\"].min()\n",
        "\n",
        "    # Add a check to prevent division by zero\n",
        "    if up + down > pd.Timedelta(0):\n",
        "        up_percentage = (up / (up + down)) * 100\n",
        "    else:\n",
        "        up_percentage = 0\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nSummary:\")\n",
        "    print(f\"Total Time: {total_time}\")\n",
        "    print(f\"Up period: {up}\")\n",
        "    print(f\"Down period: {down}\")\n",
        "    print(f\"Availability Percentage: {up_percentage}%\")\n",
        "    return up_percentage, len(compressed_intervals)\n",
        "\n",
        "# Define the worker pool\n",
        "\n",
        "# Define the date range\n",
        "start_date = datetime.strptime('2025-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "end_date = datetime.strptime('2025-08-31 23:59:59', '%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# Calculate availability every 15 days\n",
        "current_date = start_date\n",
        "result = []\n",
        "while current_date < end_date - timedelta(days=30):\n",
        "    next_date = current_date + timedelta(days=30)\n",
        "    avail,down_intervals_nb = availability(concatenated_df, workerpool_prod, current_date, next_date)\n",
        "    current_date = current_date + timedelta(days=15)\n",
        "    result.append((next_date.strftime('%Y-%m-%d'), avail,down_intervals_nb))\n",
        "\n",
        "print(result)\n",
        "\n",
        "# Plotting\n",
        "x_val = [res[0] for res in result]   # date\n",
        "y_val = [res[1] for res in result]   # availibiity ratio\n",
        "y2_val = [res[2] for res in result]  # Number of incidents\n",
        "\n",
        "# Create figure and primary axis\n",
        "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "# Plot availability percentage on primary y-axis\n",
        "color1 = '#1E90FF'  # Dodger blue for availability\n",
        "ax1.set_xlabel('Date', fontsize=12)\n",
        "ax1.set_ylabel('Availability Percentage', color=color1, fontsize=12)\n",
        "line1 = ax1.plot(x_val, y_val,\n",
        "                 color=color1,\n",
        "                 linewidth=3,\n",
        "                 linestyle='-',\n",
        "                 marker='o',\n",
        "                 markersize=10,\n",
        "                 markerfacecolor='red',\n",
        "                 markeredgecolor='darkred',\n",
        "                 markeredgewidth=2,\n",
        "                 label='Availability %')\n",
        "\n",
        "# Add value labels for availability\n",
        "for i, txt in enumerate(y_val):\n",
        "    ax1.annotate(f'{txt:.2f}%',\n",
        "                 (x_val[i], y_val[i]),\n",
        "                 xytext=(10, 10),\n",
        "                 textcoords='offset points',\n",
        "                 fontweight='bold',\n",
        "                 color='darkred')\n",
        "\n",
        "# Customize primary axis\n",
        "ax1.tick_params(axis='y', labelcolor=color1)\n",
        "ax1.axhline(y=90, color='green', linestyle='--', alpha=0.7)\n",
        "ax1.axhline(y=50, color='orange', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Create secondary y-axis for incidents\n",
        "ax2 = ax1.twinx()\n",
        "color2 = '#FF6347'  # Tomato red for incidents\n",
        "ax2.set_ylabel('Number of Incidents', color=color2, fontsize=12)\n",
        "line2 = ax2.plot(x_val, y2_val,\n",
        "                 color=color2,\n",
        "                 linewidth=3,\n",
        "                 linestyle='-',\n",
        "                 marker='s',\n",
        "                 markersize=10,\n",
        "                 markerfacecolor='orange',\n",
        "                 markeredgecolor='darkorange',\n",
        "                 label='Incidents')\n",
        "\n",
        "# Add value labels for incidents\n",
        "for i, txt in enumerate(y2_val):\n",
        "    ax2.annotate(str(txt),\n",
        "                 (x_val[i], y2_val[i]),\n",
        "                 xytext=(10, -10),\n",
        "                 textcoords='offset points',\n",
        "                 fontweight='bold',\n",
        "                 color='darkorange')\n",
        "\n",
        "# Customize secondary axis\n",
        "ax2.tick_params(axis='y', labelcolor=color2)\n",
        "\n",
        "# Title and overall formatting\n",
        "plt.title('IExec Worker Pool Availability and Incidents', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Combine legends\n",
        "lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(lines1 + lines2, labels1 + labels2, loc='best')\n",
        "\n",
        "# Improve layout\n",
        "plt.gcf().autofmt_xdate()\n",
        "plt.grid(True, linestyle=':', color='gray', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "EaZrS-L77nBl"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}